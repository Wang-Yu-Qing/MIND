{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_USER_FREQ = 20\n",
    "MIN_ITEM_FREQ = 100\n",
    "\n",
    "ratings = pd.read_csv(\"data/Books.csv\", header=None)\n",
    "ratings.columns = ['userId', 'itemId', 'rate', 'timestamp']\n",
    "\n",
    "# item filtering\n",
    "itemFreq = ratings.groupby(['itemId'])['itemId'].count()\n",
    "validSet = set(itemFreq.loc[itemFreq >= MIN_ITEM_FREQ].index)\n",
    "ratings = ratings.loc[ratings['itemId'].apply(lambda x: x in validSet), :]\n",
    "\n",
    "# user filtering\n",
    "userFreq = ratings.groupby(['userId'])['userId'].count()\n",
    "validSet = set(userFreq.loc[userFreq >= MIN_USER_FREQ].index)\n",
    "ratings = ratings.loc[ratings['userId'].apply(lambda x: x in validSet), :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode his\n",
    "ukv, ikv = list(enumerate(ratings['userId'].unique())), list(enumerate(ratings['itemId'].unique()))\n",
    "userRawId = {encId: rawId for encId, rawId in ukv}\n",
    "userEncId = {rawId: encId for encId, rawId in ukv}\n",
    "\n",
    "# encode tar, id 0 is for padding, item encode id start from 1\n",
    "itemRawId = {encId + 1: rawId for encId, rawId in ikv}\n",
    "itemEncId = {rawId: encId + 1 for encId, rawId in ikv}\n",
    "\n",
    "# 编码\n",
    "ratings['userId'] = ratings['userId'].apply(lambda x: userEncId[x])\n",
    "ratings['itemId'] = ratings['itemId'].apply(lambda x: itemEncId[x])\n",
    "\n",
    "ratings.sort_values(by=['userId', 'timestamp'], inplace=True, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "winSize = 5\n",
    "\n",
    "def padOrCut(seq, l):\n",
    "    if (len(seq) < l): return np.concatenate([seq, (l - len(seq)) * [0]])\n",
    "    # return last len\n",
    "    elif (len(seq) > l): return seq[len(seq) - l:]\n",
    "    else: return seq\n",
    "\n",
    "# split train and test users\n",
    "import random\n",
    "trainUsers, testUsers = set(), set()\n",
    "for userId in range(len(userRawId)):\n",
    "    if (random.random() <= 0.8): trainUsers.add(userId)\n",
    "    else: testUsers.add(userId)\n",
    "\n",
    "# generate user sample by sliding window\n",
    "def genUserTrainSamples(userDf):\n",
    "    # one user generate multiple train samples\n",
    "    userDf.reset_index(drop=True, inplace=True)\n",
    "    his, tar = [], []\n",
    "    for i in range(1, userDf.shape[0]): # enumerate y from 1\n",
    "        # x = window [i - winSize, i - 1], y = item[i]\n",
    "        his.append(padOrCut(userDf.iloc[max(0, i - winSize):i]['itemId'].values, winSize))\n",
    "        tar.append(userDf.iloc[i]['itemId'])\n",
    "\n",
    "    return np.stack(his), np.stack(tar)\n",
    "\n",
    "def genUserTestSamples(userDf):\n",
    "    # one user generate one test sample\n",
    "    userDf.reset_index(drop=True, inplace=True)\n",
    "    idx = int(0.8 * userDf.shape[0])\n",
    "    his = padOrCut(userDf['itemId'].iloc[:idx].values, winSize)\n",
    "    tar = userDf['itemId'].iloc[idx:].values\n",
    "\n",
    "    return his, tar\n",
    "\n",
    "boolIdx = ratings['userId'].apply(lambda x: x in trainUsers)\n",
    "trainRatings = ratings.loc[boolIdx, :]\n",
    "testRatings = ratings.loc[~boolIdx, :]\n",
    "\n",
    "trainSamples = trainRatings.groupby('userId').apply(genUserTrainSamples)\n",
    "trainHis = np.concatenate(trainSamples.apply(lambda x: x[0]).values)\n",
    "trainTar = np.concatenate(trainSamples.apply(lambda x: x[1]).values)\n",
    "\n",
    "testSamples = testRatings.groupby('userId').apply(genUserTestSamples)\n",
    "testHis = np.stack(testSamples.apply(lambda x: x[0]).values)\n",
    "_testTar = testSamples.apply(lambda x: x[1]).values\n",
    "testTar = np.arange(0, _testTar.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainHis = trainHis.astype(np.int32)\n",
    "trainTar = trainTar.astype(np.int32)\n",
    "testHis = testHis.astype(np.int32)\n",
    "testTar = testTar.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/trainHis.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(trainHis))\n",
    "\n",
    "with open(\"data/trainTar.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(trainTar))\n",
    "\n",
    "with open(\"data/testHis.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(testHis))\n",
    "\n",
    "with open(\"data/testTar.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(testTar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "winSize = 5\n",
    "\n",
    "with open(\"data/trainHis.pkl\", \"rb\") as f:\n",
    "    trainHis = pickle.loads(f.read())\n",
    "\n",
    "with open(\"data/trainTar.pkl\", \"rb\") as f:\n",
    "    trainTar = pickle.loads(f.read())\n",
    "\n",
    "with open(\"data/testHis.pkl\", \"rb\") as f:\n",
    "    testHis = pickle.loads(f.read())\n",
    "\n",
    "with open(\"data/testTar.pkl\", \"rb\") as f:\n",
    "    testTar = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, his, tar):\n",
    "        self.his = his\n",
    "        self.tar = tar\n",
    "        assert len(self.his) == len(self.tar)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.his[i], self.tar[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.his)\n",
    "\n",
    "trainData = DataLoader(\n",
    "    Dataset(trainHis, trainTar),\n",
    "    batch_size = 1028,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIND(th.nn.Module):\n",
    "    def __init__(self, D, K, R, L, nNeg, embedNum):\n",
    "        super(MIND, self).__init__()\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "        self.R = R\n",
    "        self.L = L\n",
    "        self.nNeg = nNeg\n",
    "        # weights initialization\n",
    "        self.itemEmbeds = th.nn.Embedding(embedNum, D, padding_idx=0)\n",
    "        # matmul([batch_size, k, 1, dim], [k, dim, dim']) = [batch_size, k, 1, dim']\n",
    "        self.dense1 = th.nn.Linear(D, 4 * D)\n",
    "        self.dense2 = th.nn.Linear(4 * D, D)\n",
    "        # one S for all routing operations, first dim is for batch broadcasting\n",
    "        S = th.empty(D, D)\n",
    "        th.nn.init.normal_(S, mean=0.0, std=1.0)\n",
    "        self.S = th.nn.Parameter(S) # don't forget to make S as model parameter\n",
    "        # fixed routing logits once initialized\n",
    "        self.B = th.nn.init.normal_(th.empty(K, L), mean=0.0, std=1.0)\n",
    "        self.opt = th.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    # output caps' length is in (0, 1)\n",
    "    def squash(self, caps, bs):\n",
    "        n = th.norm(caps, dim=2).view(bs, self.K, 1)\n",
    "        nSquare = th.pow(n, 2)\n",
    "\n",
    "        return (nSquare / ((1 + nSquare) * n + 1e-9)) * caps\n",
    "    \n",
    "    def B2IRouting(self, his, bs):\n",
    "        \"\"\"B2I dynamic routing, input behaviors, output caps\n",
    "        \"\"\"\n",
    "        # init b, bji = b[j][i] rather than b[i][j] for matmul convinience\n",
    "        # no grad for b: https://github.com/Ugenteraan/CapsNet-PyTorch/blob/master/CapsNet-PyTorch.ipynb\n",
    "        B = self.B.detach()\n",
    "        # except for first routing round, each sample's w is different, so need a dim for batch\n",
    "        B = th.tile(B, (bs, 1, 1)) # (bs, K, L)\n",
    "\n",
    "        # masking, make padding indices' routing logit as INT_MAX so that softmax result is 0\n",
    "        # (bs, L) -> (bs, 1, L) -> (bs, K, L)\n",
    "        mask = (his != 0).unsqueeze(1).tile(1, self.K, 1)\n",
    "        drop = th.ones_like(mask) * -(1 << 31)\n",
    "\n",
    "        his = self.itemEmbeds(his) # (bs, L, D)\n",
    "        his = th.matmul(his, self.S)\n",
    "\n",
    "        for i in range(self.R):\n",
    "            BMasked = th.where(mask, B, drop)\n",
    "            W = th.softmax(BMasked, dim=2) # (bs, K, L)\n",
    "            if i < self.R - 1:\n",
    "                with th.no_grad():\n",
    "                    # weighted sum all i to each j\n",
    "                    caps = th.matmul(W, his) # (bs, K, D)\n",
    "                    caps = self.squash(caps, bs)\n",
    "                    B += th.matmul(caps, th.transpose(his, 1, 2)) # (bs, K, L)\n",
    "            else:\n",
    "                caps = th.matmul(W, his) # (bs, K, D)\n",
    "                caps = self.squash(caps, bs)\n",
    "                # skip routing logits update in last round\n",
    "\n",
    "        # mlp\n",
    "        caps = self.dense2(th.relu(self.dense1(caps))) # (bs, K, D)\n",
    "        ## l2 norm\n",
    "        #caps = caps / (th.norm(caps, dim=2).view(bs, self.K, 1) + 1e-9)\n",
    "        \n",
    "        return caps\n",
    "    \n",
    "    def labelAwareAttation(self, caps, tar, p=2):\n",
    "        \"\"\"label-aware attention, input caps and targets, output logits\n",
    "            caps: (bs, K, D)\n",
    "            tar: (bs, cnt, D)\n",
    "            for postive tar, cnt = 1\n",
    "            for negative tar, cnt = self.nNeg\n",
    "        \"\"\"\n",
    "        tar = tar.transpose(1, 2) # (bs, D, cnt)\n",
    "        w = th.softmax(\n",
    "                # (bs, K, D) X (bs, D, cnt) -> (bs, K, cnt) -> (bs, cnt, K)\n",
    "                th.pow(th.transpose(th.matmul(caps, tar), 1, 2), p),\n",
    "                dim=2\n",
    "            )\n",
    "        w = w.unsqueeze(2) # (bs, cnt, K) -> (bs, cnt, 1, K)\n",
    "\n",
    "        # (bs, cnt, 1, K) X (bs, 1, K, D) -> (bs, cnt, 1, D) -> (bs, cnt, D)\n",
    "        caps = th.matmul(w, caps.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "        return caps\n",
    "\n",
    "    def sampledSoftmax(self, caps, tar, bs, tmp=0.01):\n",
    "        tarPos = self.itemEmbeds(tar) # (bs, D)\n",
    "        capsPos = self.labelAwareAttation(caps, tarPos.unsqueeze(1)).squeeze(1) # (bs, D)\n",
    "        # pos logits\n",
    "        #his = his / (th.norm(his, dim=1).view(bs, 1) + 1e-9)\n",
    "        #tar = tar / (th.norm(tar, dim=1).view(bs, 1) + 1e-9)\n",
    "        # (bs, D) dot (bs, D) -> (bs, D) - sum > (bs, )\n",
    "        posLogits = th.sigmoid(th.sum(capsPos * tarPos, dim=1) / tmp)\n",
    "\n",
    "        # neg logits\n",
    "        # in-batch negative sampling\n",
    "        tarNeg = tarPos[th.multinomial(th.ones(bs), self.nNeg * bs, replacement=True)].view(bs, self.nNeg, self.D) # (batch_size, nNeg, D)\n",
    "        capsNeg = self.labelAwareAttation(caps, tarNeg)\n",
    "        # hisNeg[b][i].dot(tarNeg[b][i]) for all b, i\n",
    "        negLogits = th.sigmoid(th.sum(capsNeg * tarNeg, dim=2).view(bs * self.nNeg) / tmp)\n",
    "\n",
    "        logits = th.concat([posLogits, negLogits])\n",
    "        labels = th.concat([th.ones(bs, ), th.zeros(bs * self.nNeg)])\n",
    "\n",
    "        return logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Step 00000 | Loss 34.365261\n",
      "Epoch 00 | Step 00200 | Loss 6.171427\n",
      "Epoch 00 | Step 00400 | Loss 3.469577\n",
      "Epoch 00 | Step 00600 | Loss 2.552852\n",
      "Epoch 00 | Step 00800 | Loss 2.091877\n",
      "Epoch 00 | Step 01000 | Loss 1.814004\n",
      "Epoch 00 | Step 01200 | Loss 1.628222\n",
      "Epoch 00 | Step 01400 | Loss 1.494917\n",
      "Epoch 00 | Step 01600 | Loss 1.393998\n",
      "Epoch 00 | Step 01800 | Loss 1.313630\n",
      "Epoch 00 | Step 02000 | Loss 1.246513\n",
      "Epoch 00 | Step 02200 | Loss 1.188039\n",
      "Epoch 00 | Step 02400 | Loss 1.134712\n",
      "Epoch 01 | Step 00000 | Loss 0.476691\n",
      "Epoch 01 | Step 00200 | Loss 0.433980\n",
      "Epoch 01 | Step 00400 | Loss 0.404568\n",
      "Epoch 01 | Step 00600 | Loss 0.378373\n",
      "Epoch 01 | Step 00800 | Loss 0.355430\n",
      "Epoch 01 | Step 01000 | Loss 0.334663\n",
      "Epoch 01 | Step 01200 | Loss 0.316372\n",
      "Epoch 01 | Step 01400 | Loss 0.300241\n",
      "Epoch 01 | Step 01600 | Loss 0.285924\n",
      "Epoch 01 | Step 01800 | Loss 0.273332\n",
      "Epoch 01 | Step 02000 | Loss 0.262174\n",
      "Epoch 01 | Step 02200 | Loss 0.252118\n",
      "Epoch 01 | Step 02400 | Loss 0.242910\n",
      "Epoch 02 | Step 00000 | Loss 0.131172\n",
      "Epoch 02 | Step 00200 | Loss 0.130565\n",
      "Epoch 02 | Step 00400 | Loss 0.127762\n",
      "Epoch 02 | Step 00600 | Loss 0.126092\n",
      "Epoch 02 | Step 00800 | Loss 0.124106\n",
      "Epoch 02 | Step 01000 | Loss 0.122307\n",
      "Epoch 02 | Step 01200 | Loss 0.120745\n",
      "Epoch 02 | Step 01400 | Loss 0.119398\n",
      "Epoch 02 | Step 01600 | Loss 0.118057\n",
      "Epoch 02 | Step 01800 | Loss 0.116827\n",
      "Epoch 02 | Step 02000 | Loss 0.115762\n",
      "Epoch 02 | Step 02200 | Loss 0.114789\n",
      "Epoch 02 | Step 02400 | Loss 0.113876\n",
      "Epoch 03 | Step 00000 | Loss 0.101114\n",
      "Epoch 03 | Step 00200 | Loss 0.102318\n",
      "Epoch 03 | Step 00400 | Loss 0.102007\n",
      "Epoch 03 | Step 00600 | Loss 0.101840\n",
      "Epoch 03 | Step 00800 | Loss 0.101589\n",
      "Epoch 03 | Step 01000 | Loss 0.101362\n",
      "Epoch 03 | Step 01200 | Loss 0.101223\n",
      "Epoch 03 | Step 01400 | Loss 0.101053\n",
      "Epoch 03 | Step 01600 | Loss 0.100870\n",
      "Epoch 03 | Step 01800 | Loss 0.100725\n",
      "Epoch 03 | Step 02000 | Loss 0.100565\n",
      "Epoch 03 | Step 02200 | Loss 0.100411\n",
      "Epoch 03 | Step 02400 | Loss 0.100301\n",
      "Epoch 04 | Step 00000 | Loss 0.097140\n",
      "Epoch 04 | Step 00200 | Loss 0.098609\n",
      "Epoch 04 | Step 00400 | Loss 0.098442\n",
      "Epoch 04 | Step 00600 | Loss 0.098367\n",
      "Epoch 04 | Step 00800 | Loss 0.098252\n",
      "Epoch 04 | Step 01000 | Loss 0.098289\n",
      "Epoch 04 | Step 01200 | Loss 0.098262\n",
      "Epoch 04 | Step 01400 | Loss 0.098262\n",
      "Epoch 04 | Step 01600 | Loss 0.098226\n",
      "Epoch 04 | Step 01800 | Loss 0.098195\n",
      "Epoch 04 | Step 02000 | Loss 0.098143\n",
      "Epoch 04 | Step 02200 | Loss 0.098110\n",
      "Epoch 04 | Step 02400 | Loss 0.098066\n",
      "Epoch 05 | Step 00000 | Loss 0.096786\n",
      "Epoch 05 | Step 00200 | Loss 0.097559\n",
      "Epoch 05 | Step 00400 | Loss 0.097525\n",
      "Epoch 05 | Step 00600 | Loss 0.097484\n",
      "Epoch 05 | Step 00800 | Loss 0.097483\n",
      "Epoch 05 | Step 01000 | Loss 0.097458\n",
      "Epoch 05 | Step 01200 | Loss 0.097459\n",
      "Epoch 05 | Step 01400 | Loss 0.097473\n",
      "Epoch 05 | Step 01600 | Loss 0.097448\n",
      "Epoch 05 | Step 01800 | Loss 0.097420\n",
      "Epoch 05 | Step 02000 | Loss 0.097401\n",
      "Epoch 05 | Step 02200 | Loss 0.097377\n",
      "Epoch 05 | Step 02400 | Loss 0.097351\n",
      "Epoch 06 | Step 00000 | Loss 0.096793\n",
      "Epoch 06 | Step 00200 | Loss 0.097256\n",
      "Epoch 06 | Step 00400 | Loss 0.097219\n",
      "Epoch 06 | Step 00600 | Loss 0.097175\n",
      "Epoch 06 | Step 00800 | Loss 0.097128\n",
      "Epoch 06 | Step 01000 | Loss 0.097114\n",
      "Epoch 06 | Step 01200 | Loss 0.097098\n",
      "Epoch 06 | Step 01400 | Loss 0.097072\n",
      "Epoch 06 | Step 01600 | Loss 0.097064\n",
      "Epoch 06 | Step 01800 | Loss 0.097036\n",
      "Epoch 06 | Step 02000 | Loss 0.097021\n",
      "Epoch 06 | Step 02200 | Loss 0.097022\n",
      "Epoch 06 | Step 02400 | Loss 0.097022\n",
      "Epoch 07 | Step 00000 | Loss 0.096730\n",
      "Epoch 07 | Step 00200 | Loss 0.096841\n",
      "Epoch 07 | Step 00400 | Loss 0.096941\n",
      "Epoch 07 | Step 00600 | Loss 0.096907\n",
      "Epoch 07 | Step 00800 | Loss 0.096903\n",
      "Epoch 07 | Step 01000 | Loss 0.096881\n",
      "Epoch 07 | Step 01200 | Loss 0.096864\n",
      "Epoch 07 | Step 01400 | Loss 0.096860\n",
      "Epoch 07 | Step 01600 | Loss 0.096863\n",
      "Epoch 07 | Step 01800 | Loss 0.096869\n",
      "Epoch 07 | Step 02000 | Loss 0.096864\n",
      "Epoch 07 | Step 02200 | Loss 0.096860\n",
      "Epoch 07 | Step 02400 | Loss 0.096857\n",
      "Epoch 08 | Step 00000 | Loss 0.096576\n",
      "Epoch 08 | Step 00200 | Loss 0.096802\n",
      "Epoch 08 | Step 00400 | Loss 0.096889\n",
      "Epoch 08 | Step 00600 | Loss 0.096832\n",
      "Epoch 08 | Step 00800 | Loss 0.096819\n",
      "Epoch 08 | Step 01000 | Loss 0.096794\n",
      "Epoch 08 | Step 01200 | Loss 0.096781\n",
      "Epoch 08 | Step 01400 | Loss 0.096789\n",
      "Epoch 08 | Step 01600 | Loss 0.096776\n",
      "Epoch 08 | Step 01800 | Loss 0.096776\n",
      "Epoch 08 | Step 02000 | Loss 0.096779\n",
      "Epoch 08 | Step 02200 | Loss 0.096775\n",
      "Epoch 08 | Step 02400 | Loss 0.096769\n",
      "Epoch 09 | Step 00000 | Loss 0.096492\n",
      "Epoch 09 | Step 00200 | Loss 0.096723\n",
      "Epoch 09 | Step 00400 | Loss 0.096710\n",
      "Epoch 09 | Step 00600 | Loss 0.096725\n",
      "Epoch 09 | Step 00800 | Loss 0.096719\n",
      "Epoch 09 | Step 01000 | Loss 0.096729\n",
      "Epoch 09 | Step 01200 | Loss 0.096722\n",
      "Epoch 09 | Step 01400 | Loss 0.096722\n",
      "Epoch 09 | Step 01600 | Loss 0.096715\n",
      "Epoch 09 | Step 01800 | Loss 0.096712\n",
      "Epoch 09 | Step 02000 | Loss 0.096708\n",
      "Epoch 09 | Step 02200 | Loss 0.096707\n",
      "Epoch 09 | Step 02400 | Loss 0.096707\n",
      "Epoch 10 | Step 00000 | Loss 0.096606\n",
      "Epoch 10 | Step 00200 | Loss 0.096681\n",
      "Epoch 10 | Step 00400 | Loss 0.096736\n",
      "Epoch 10 | Step 00600 | Loss 0.096694\n",
      "Epoch 10 | Step 00800 | Loss 0.096699\n",
      "Epoch 10 | Step 01000 | Loss 0.096689\n",
      "Epoch 10 | Step 01200 | Loss 0.096680\n",
      "Epoch 10 | Step 01400 | Loss 0.096679\n",
      "Epoch 10 | Step 01600 | Loss 0.096673\n",
      "Epoch 10 | Step 01800 | Loss 0.096667\n",
      "Epoch 10 | Step 02000 | Loss 0.096667\n",
      "Epoch 10 | Step 02200 | Loss 0.096667\n",
      "Epoch 10 | Step 02400 | Loss 0.096663\n",
      "Epoch 11 | Step 00000 | Loss 0.096407\n",
      "Epoch 11 | Step 00200 | Loss 0.096601\n",
      "Epoch 11 | Step 00400 | Loss 0.096650\n",
      "Epoch 11 | Step 00600 | Loss 0.096641\n",
      "Epoch 11 | Step 00800 | Loss 0.096632\n",
      "Epoch 11 | Step 01000 | Loss 0.096627\n",
      "Epoch 11 | Step 01200 | Loss 0.096625\n",
      "Epoch 11 | Step 01400 | Loss 0.096617\n",
      "Epoch 11 | Step 01600 | Loss 0.096609\n",
      "Epoch 11 | Step 01800 | Loss 0.096614\n",
      "Epoch 11 | Step 02000 | Loss 0.096604\n",
      "Epoch 11 | Step 02200 | Loss 0.096606\n",
      "Epoch 11 | Step 02400 | Loss 0.096599\n",
      "Epoch 12 | Step 00000 | Loss 0.096535\n",
      "Epoch 12 | Step 00200 | Loss 0.096516\n",
      "Epoch 12 | Step 00400 | Loss 0.096481\n",
      "Epoch 12 | Step 00600 | Loss 0.096451\n",
      "Epoch 12 | Step 00800 | Loss 0.096432\n",
      "Epoch 12 | Step 01000 | Loss 0.096405\n",
      "Epoch 12 | Step 01200 | Loss 0.096382\n",
      "Epoch 12 | Step 01400 | Loss 0.096349\n",
      "Epoch 12 | Step 01600 | Loss 0.096305\n",
      "Epoch 12 | Step 01800 | Loss 0.096244\n",
      "Epoch 12 | Step 02000 | Loss 0.096174\n",
      "Epoch 12 | Step 02200 | Loss 0.096087\n",
      "Epoch 12 | Step 02400 | Loss 0.095979\n",
      "Epoch 13 | Step 00000 | Loss 0.094041\n",
      "Epoch 13 | Step 00200 | Loss 0.093931\n",
      "Epoch 13 | Step 00400 | Loss 0.093673\n",
      "Epoch 13 | Step 00600 | Loss 0.093393\n",
      "Epoch 13 | Step 00800 | Loss 0.093113\n",
      "Epoch 13 | Step 01000 | Loss 0.092828\n",
      "Epoch 13 | Step 01200 | Loss 0.092530\n",
      "Epoch 13 | Step 01400 | Loss 0.092235\n",
      "Epoch 13 | Step 01600 | Loss 0.091946\n",
      "Epoch 13 | Step 01800 | Loss 0.091666\n",
      "Epoch 13 | Step 02000 | Loss 0.091399\n",
      "Epoch 13 | Step 02200 | Loss 0.091125\n",
      "Epoch 13 | Step 02400 | Loss 0.090868\n",
      "Epoch 14 | Step 00000 | Loss 0.087413\n",
      "Epoch 14 | Step 00200 | Loss 0.087178\n",
      "Epoch 14 | Step 00400 | Loss 0.087035\n",
      "Epoch 14 | Step 00600 | Loss 0.086894\n",
      "Epoch 14 | Step 00800 | Loss 0.086740\n",
      "Epoch 14 | Step 01000 | Loss 0.086599\n",
      "Epoch 14 | Step 01200 | Loss 0.086464\n",
      "Epoch 14 | Step 01400 | Loss 0.086331\n",
      "Epoch 14 | Step 01600 | Loss 0.086220\n",
      "Epoch 14 | Step 01800 | Loss 0.086101\n",
      "Epoch 14 | Step 02000 | Loss 0.085999\n",
      "Epoch 14 | Step 02200 | Loss 0.085896\n",
      "Epoch 14 | Step 02400 | Loss 0.085801\n",
      "Epoch 15 | Step 00000 | Loss 0.084515\n",
      "Epoch 15 | Step 00200 | Loss 0.084355\n",
      "Epoch 15 | Step 00400 | Loss 0.084334\n",
      "Epoch 15 | Step 00600 | Loss 0.084243\n",
      "Epoch 15 | Step 00800 | Loss 0.084196\n",
      "Epoch 15 | Step 01000 | Loss 0.084134\n",
      "Epoch 15 | Step 01200 | Loss 0.084081\n",
      "Epoch 15 | Step 01400 | Loss 0.084034\n",
      "Epoch 15 | Step 01600 | Loss 0.084005\n",
      "Epoch 15 | Step 01800 | Loss 0.083967\n",
      "Epoch 15 | Step 02000 | Loss 0.083938\n",
      "Epoch 15 | Step 02200 | Loss 0.083894\n",
      "Epoch 15 | Step 02400 | Loss 0.083858\n",
      "Epoch 16 | Step 00000 | Loss 0.085160\n",
      "Epoch 16 | Step 00200 | Loss 0.083309\n",
      "Epoch 16 | Step 00400 | Loss 0.083291\n",
      "Epoch 16 | Step 00600 | Loss 0.083293\n",
      "Epoch 16 | Step 00800 | Loss 0.083266\n",
      "Epoch 16 | Step 01000 | Loss 0.083266\n",
      "Epoch 16 | Step 01200 | Loss 0.083242\n",
      "Epoch 16 | Step 01400 | Loss 0.083222\n",
      "Epoch 16 | Step 01600 | Loss 0.083194\n",
      "Epoch 16 | Step 01800 | Loss 0.083179\n",
      "Epoch 16 | Step 02000 | Loss 0.083170\n",
      "Epoch 16 | Step 02200 | Loss 0.083161\n",
      "Epoch 16 | Step 02400 | Loss 0.083142\n",
      "Epoch 17 | Step 00000 | Loss 0.082455\n",
      "Epoch 17 | Step 00200 | Loss 0.082832\n",
      "Epoch 17 | Step 00400 | Loss 0.082841\n",
      "Epoch 17 | Step 00600 | Loss 0.082803\n",
      "Epoch 17 | Step 00800 | Loss 0.082810\n",
      "Epoch 17 | Step 01000 | Loss 0.082812\n",
      "Epoch 17 | Step 01200 | Loss 0.082804\n",
      "Epoch 17 | Step 01400 | Loss 0.082815\n",
      "Epoch 17 | Step 01600 | Loss 0.082802\n",
      "Epoch 17 | Step 01800 | Loss 0.082810\n",
      "Epoch 17 | Step 02000 | Loss 0.082794\n",
      "Epoch 17 | Step 02200 | Loss 0.082783\n",
      "Epoch 17 | Step 02400 | Loss 0.082773\n",
      "Epoch 18 | Step 00000 | Loss 0.082174\n",
      "Epoch 18 | Step 00200 | Loss 0.082501\n",
      "Epoch 18 | Step 00400 | Loss 0.082527\n",
      "Epoch 18 | Step 00600 | Loss 0.082531\n",
      "Epoch 18 | Step 00800 | Loss 0.082542\n",
      "Epoch 18 | Step 01000 | Loss 0.082544\n",
      "Epoch 18 | Step 01200 | Loss 0.082566\n",
      "Epoch 18 | Step 01400 | Loss 0.082555\n",
      "Epoch 18 | Step 01600 | Loss 0.082552\n",
      "Epoch 18 | Step 01800 | Loss 0.082554\n",
      "Epoch 18 | Step 02000 | Loss 0.082559\n",
      "Epoch 18 | Step 02200 | Loss 0.082569\n",
      "Epoch 18 | Step 02400 | Loss 0.082564\n",
      "Epoch 19 | Step 00000 | Loss 0.083184\n",
      "Epoch 19 | Step 00200 | Loss 0.082482\n",
      "Epoch 19 | Step 00400 | Loss 0.082420\n",
      "Epoch 19 | Step 00600 | Loss 0.082399\n",
      "Epoch 19 | Step 00800 | Loss 0.082405\n",
      "Epoch 19 | Step 01000 | Loss 0.082403\n",
      "Epoch 19 | Step 01200 | Loss 0.082399\n",
      "Epoch 19 | Step 01400 | Loss 0.082411\n",
      "Epoch 19 | Step 01600 | Loss 0.082414\n",
      "Epoch 19 | Step 01800 | Loss 0.082419\n",
      "Epoch 19 | Step 02000 | Loss 0.082411\n",
      "Epoch 19 | Step 02200 | Loss 0.082407\n",
      "Epoch 19 | Step 02400 | Loss 0.082410\n",
      "Epoch 20 | Step 00000 | Loss 0.082892\n",
      "Epoch 20 | Step 00200 | Loss 0.082315\n",
      "Epoch 20 | Step 00400 | Loss 0.082311\n",
      "Epoch 20 | Step 00600 | Loss 0.082278\n",
      "Epoch 20 | Step 00800 | Loss 0.082287\n",
      "Epoch 20 | Step 01000 | Loss 0.082288\n",
      "Epoch 20 | Step 01200 | Loss 0.082295\n",
      "Epoch 20 | Step 01400 | Loss 0.082297\n",
      "Epoch 20 | Step 01600 | Loss 0.082295\n",
      "Epoch 20 | Step 01800 | Loss 0.082286\n",
      "Epoch 20 | Step 02000 | Loss 0.082284\n",
      "Epoch 20 | Step 02200 | Loss 0.082287\n",
      "Epoch 20 | Step 02400 | Loss 0.082286\n",
      "Epoch 21 | Step 00000 | Loss 0.082586\n",
      "Epoch 21 | Step 00200 | Loss 0.082059\n",
      "Epoch 21 | Step 00400 | Loss 0.082093\n",
      "Epoch 21 | Step 00600 | Loss 0.082106\n",
      "Epoch 21 | Step 00800 | Loss 0.082119\n",
      "Epoch 21 | Step 01000 | Loss 0.082138\n",
      "Epoch 21 | Step 01200 | Loss 0.082129\n",
      "Epoch 21 | Step 01400 | Loss 0.082122\n",
      "Epoch 21 | Step 01600 | Loss 0.082146\n",
      "Epoch 21 | Step 01800 | Loss 0.082138\n",
      "Epoch 21 | Step 02000 | Loss 0.082132\n",
      "Epoch 21 | Step 02200 | Loss 0.082119\n",
      "Epoch 21 | Step 02400 | Loss 0.082106\n",
      "Epoch 22 | Step 00000 | Loss 0.082612\n",
      "Epoch 22 | Step 00200 | Loss 0.081892\n",
      "Epoch 22 | Step 00400 | Loss 0.081866\n",
      "Epoch 22 | Step 00600 | Loss 0.081878\n",
      "Epoch 22 | Step 00800 | Loss 0.081923\n",
      "Epoch 22 | Step 01000 | Loss 0.081904\n",
      "Epoch 22 | Step 01200 | Loss 0.081892\n",
      "Epoch 22 | Step 01400 | Loss 0.081883\n",
      "Epoch 22 | Step 01600 | Loss 0.081874\n",
      "Epoch 22 | Step 01800 | Loss 0.081863\n",
      "Epoch 22 | Step 02000 | Loss 0.081842\n",
      "Epoch 22 | Step 02200 | Loss 0.081834\n",
      "Epoch 22 | Step 02400 | Loss 0.081826\n",
      "Epoch 23 | Step 00000 | Loss 0.081526\n",
      "Epoch 23 | Step 00200 | Loss 0.081587\n",
      "Epoch 23 | Step 00400 | Loss 0.081577\n",
      "Epoch 23 | Step 00600 | Loss 0.081565\n",
      "Epoch 23 | Step 00800 | Loss 0.081558\n",
      "Epoch 23 | Step 01000 | Loss 0.081589\n",
      "Epoch 23 | Step 01200 | Loss 0.081584\n",
      "Epoch 23 | Step 01400 | Loss 0.081572\n",
      "Epoch 23 | Step 01600 | Loss 0.081557\n",
      "Epoch 23 | Step 01800 | Loss 0.081543\n",
      "Epoch 23 | Step 02000 | Loss 0.081526\n",
      "Epoch 23 | Step 02200 | Loss 0.081512\n",
      "Epoch 23 | Step 02400 | Loss 0.081500\n",
      "Epoch 24 | Step 00000 | Loss 0.080322\n",
      "Epoch 24 | Step 00200 | Loss 0.081232\n",
      "Epoch 24 | Step 00400 | Loss 0.081231\n",
      "Epoch 24 | Step 00600 | Loss 0.081262\n",
      "Epoch 24 | Step 00800 | Loss 0.081230\n",
      "Epoch 24 | Step 01000 | Loss 0.081218\n",
      "Epoch 24 | Step 01200 | Loss 0.081219\n",
      "Epoch 24 | Step 01400 | Loss 0.081217\n",
      "Epoch 24 | Step 01600 | Loss 0.081212\n",
      "Epoch 24 | Step 01800 | Loss 0.081214\n",
      "Epoch 24 | Step 02000 | Loss 0.081211\n",
      "Epoch 24 | Step 02200 | Loss 0.081200\n",
      "Epoch 24 | Step 02400 | Loss 0.081193\n",
      "Epoch 25 | Step 00000 | Loss 0.080974\n",
      "Epoch 25 | Step 00200 | Loss 0.081007\n",
      "Epoch 25 | Step 00400 | Loss 0.081029\n",
      "Epoch 25 | Step 00600 | Loss 0.081025\n",
      "Epoch 25 | Step 00800 | Loss 0.081019\n",
      "Epoch 25 | Step 01000 | Loss 0.081011\n",
      "Epoch 25 | Step 01200 | Loss 0.080997\n",
      "Epoch 25 | Step 01400 | Loss 0.080995\n",
      "Epoch 25 | Step 01600 | Loss 0.080988\n",
      "Epoch 25 | Step 01800 | Loss 0.080985\n",
      "Epoch 25 | Step 02000 | Loss 0.080983\n",
      "Epoch 25 | Step 02200 | Loss 0.080979\n",
      "Epoch 25 | Step 02400 | Loss 0.080973\n",
      "Epoch 26 | Step 00000 | Loss 0.081000\n",
      "Epoch 26 | Step 00200 | Loss 0.080773\n",
      "Epoch 26 | Step 00400 | Loss 0.080792\n",
      "Epoch 26 | Step 00600 | Loss 0.080811\n",
      "Epoch 26 | Step 00800 | Loss 0.080812\n",
      "Epoch 26 | Step 01000 | Loss 0.080801\n",
      "Epoch 26 | Step 01200 | Loss 0.080797\n",
      "Epoch 26 | Step 01400 | Loss 0.080802\n",
      "Epoch 26 | Step 01600 | Loss 0.080796\n",
      "Epoch 26 | Step 01800 | Loss 0.080793\n",
      "Epoch 26 | Step 02000 | Loss 0.080791\n",
      "Epoch 26 | Step 02200 | Loss 0.080787\n",
      "Epoch 26 | Step 02400 | Loss 0.080779\n",
      "Epoch 27 | Step 00000 | Loss 0.080440\n",
      "Epoch 27 | Step 00200 | Loss 0.080598\n",
      "Epoch 27 | Step 00400 | Loss 0.080632\n",
      "Epoch 27 | Step 00600 | Loss 0.080638\n",
      "Epoch 27 | Step 00800 | Loss 0.080630\n",
      "Epoch 27 | Step 01000 | Loss 0.080617\n",
      "Epoch 27 | Step 01200 | Loss 0.080630\n",
      "Epoch 27 | Step 01400 | Loss 0.080638\n",
      "Epoch 27 | Step 01600 | Loss 0.080641\n",
      "Epoch 27 | Step 01800 | Loss 0.080631\n",
      "Epoch 27 | Step 02000 | Loss 0.080628\n",
      "Epoch 27 | Step 02200 | Loss 0.080618\n",
      "Epoch 27 | Step 02400 | Loss 0.080615\n",
      "Epoch 28 | Step 00000 | Loss 0.081435\n",
      "Epoch 28 | Step 00200 | Loss 0.080509\n",
      "Epoch 28 | Step 00400 | Loss 0.080499\n",
      "Epoch 28 | Step 00600 | Loss 0.080495\n",
      "Epoch 28 | Step 00800 | Loss 0.080486\n",
      "Epoch 28 | Step 01000 | Loss 0.080489\n",
      "Epoch 28 | Step 01200 | Loss 0.080485\n",
      "Epoch 28 | Step 01400 | Loss 0.080494\n",
      "Epoch 28 | Step 01600 | Loss 0.080495\n",
      "Epoch 28 | Step 01800 | Loss 0.080490\n",
      "Epoch 28 | Step 02000 | Loss 0.080489\n",
      "Epoch 28 | Step 02200 | Loss 0.080484\n",
      "Epoch 28 | Step 02400 | Loss 0.080483\n",
      "Epoch 29 | Step 00000 | Loss 0.080978\n",
      "Epoch 29 | Step 00200 | Loss 0.080321\n",
      "Epoch 29 | Step 00400 | Loss 0.080313\n",
      "Epoch 29 | Step 00600 | Loss 0.080332\n",
      "Epoch 29 | Step 00800 | Loss 0.080355\n",
      "Epoch 29 | Step 01000 | Loss 0.080362\n",
      "Epoch 29 | Step 01200 | Loss 0.080355\n",
      "Epoch 29 | Step 01400 | Loss 0.080349\n",
      "Epoch 29 | Step 01600 | Loss 0.080341\n",
      "Epoch 29 | Step 01800 | Loss 0.080342\n",
      "Epoch 29 | Step 02000 | Loss 0.080342\n",
      "Epoch 29 | Step 02200 | Loss 0.080347\n",
      "Epoch 29 | Step 02400 | Loss 0.080341\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 30\n",
    "model = MIND(D=8, K=3, R=3, L=winSize, nNeg=50, embedNum=len(itemEncId) + 1)\n",
    "BCELoss = th.nn.BCELoss()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epochTotalLoss = 0\n",
    "    for step, (his, tar) in enumerate(trainData):\n",
    "        bs = his.shape[0]\n",
    "        caps = model.B2IRouting(his, bs)\n",
    "        logits, labels = model.sampledSoftmax(caps, tar, bs)\n",
    "\n",
    "        loss = BCELoss(logits, labels)\n",
    "        loss.backward()\n",
    "        model.opt.step()\n",
    "        model.opt.zero_grad()\n",
    "        epochTotalLoss += loss\n",
    "        if (step % 200 == 0):\n",
    "            print('Epoch {:02d} | Step {:05d} | Loss {:.6f}'.format(\n",
    "                epoch,\n",
    "                step,\n",
    "                epochTotalLoss / (step + 1),\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1616/1616 [00:15<00:00, 105.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@30: 0.01786042417782658, hitRate@30: 0.17640254482359746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "testData = DataLoader(\n",
    "    Dataset(testHis, testTar),\n",
    "    batch_size = 8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "with th.no_grad():\n",
    "    ie = model.itemEmbeds.weight\n",
    "    #ie /= th.norm(ie, dim=1).view(ie.shape[0], 1) + 1e-9\n",
    "    N, top = ie.shape[0], 30\n",
    "    # user-wise recall(0~1) and hit (0 or 1)\n",
    "    recalls, hitRates = [], []\n",
    "    for his, tar in tqdm.tqdm(testData):\n",
    "        bs = his.shape[0]\n",
    "        caps = model.B2IRouting(his, bs) # (bs, K, D)\n",
    "\n",
    "        # topN for all K caps's logits\n",
    "        # (bs, K, D) X (bs, D, N) -> (bs, K, N) -> (bs, K * N)\n",
    "        logits = th.matmul(caps, th.transpose(ie, 0, 1)).view(bs, model.K * N).detach().numpy()\n",
    "        # quick select over dim 1\n",
    "        res = np.argpartition(logits, kth=N - top, axis=1)[:, -top:] # (bs, top)\n",
    "        hits = 0\n",
    "        for r, t in zip(res, tar):\n",
    "            t = [x for x in _testTar[t] if x != 0]\n",
    "            if not t: continue\n",
    "            r = set(r)\n",
    "            for i in t:\n",
    "                if (i in r): hits += 1\n",
    "            recalls.append(hits / len(t))\n",
    "            hitRates.append(1 if hits > 0 else 0)\n",
    "\n",
    "    print(f\"recall@{top}: {np.mean(recalls)}, hitRate@{top}: {np.mean(hitRates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型过程实验代码 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "K = 4\n",
    "R = 1\n",
    "L = 3\n",
    "\n",
    "itemEmbeds = th.nn.Embedding(len(itemEncId), D, padding_idx=0)\n",
    "\n",
    "\"\"\"\n",
    "    Get number of interest number using equation (9) in the paper\n",
    "    @x: (batch_size, seq_len), input batch user history item seq\n",
    "    @K: basic interest number\n",
    "\n",
    "    @output: (batch_size, )\n",
    "\"\"\"\n",
    "def getK(x, K):\n",
    "    return th.maximum(th.minimum(th.log2(x.count_nonzero(dim=1)), th.tensor([K])), th.tensor([1])).type(th.int8)\n",
    "\n",
    "\"\"\"\n",
    "    squash function using equation (7) in the paper\n",
    "    @caps: (batch_size, k, dim), interest capsules\n",
    "    \n",
    "    @output: (batch_size, k, dim)\n",
    "\"\"\"\n",
    "def squash(caps):\n",
    "    l2Norm = th.norm(caps, dim=2) # (batch_size, k)\n",
    "    l2NormSqure = th.pow(l2Norm, 2)\n",
    "\n",
    "    return (l2NormSqure / (1 + l2NormSqure)).view(bs, K, 1) * (caps / l2Norm.view(bs, K, 1))\n",
    "\n",
    "# weights initialization, \n",
    "# init b, bji = b[j][i]\n",
    "b = th.empty(K, L)\n",
    "th.nn.init.normal_(b, mean=0.0, std=1.0)\n",
    "# one S for all routing operations, first dim is for batch broadcasting\n",
    "S = th.empty(D, D)\n",
    "th.nn.init.normal_(S, mean=0.0, std=1.0)\n",
    "\n",
    "his = th.tensor([[1, 2, 0], [3, 2, 0]])\n",
    "tar = th.tensor([3, 1])\n",
    "batch_labels = th.tensor([1, 0])\n",
    "\n",
    "# B2I dynamic routing, input behaviors, output caps\n",
    "bs = his.shape[0]\n",
    "# k is fixed for batch forward, cannot find a way to use variant k with batch\n",
    "#k = getK(his, K) \n",
    "I = itemEmbeds(his) # (batch_size, len, dim)\n",
    "for i in range(R):\n",
    "    w = th.softmax(b, dim=1) # (K, L)\n",
    "    I = th.matmul(I, S) # (batch_size, len, dim), bilinear transform\n",
    "    caps = squash(th.matmul(w, I)) # (batch_size, K, dim)\n",
    "    _b = th.matmul(caps, th.transpose(I, 1, 2)) # (batch_size, K, L), _bji = _b[j][i]\n",
    "    # sum over batch dim first, then add to b\n",
    "    b += th.sum(_b, dim=0) # (K, L)\n",
    "\n",
    "# label-aware attention, input caps and targets, output logits\n",
    "tar = itemEmbeds(tar) # (batch_size, dim)\n",
    "# in-batch negative sampling\n",
    "\"\"\"\n",
    "pos:\n",
    "            caps                     y                  weights\n",
    "    (batch_size, K, dim) * (batch_size, dim, 1) = (batch_size, K, 1)\n",
    "\n",
    "            weights                caps\n",
    "    (batch_size, 1, K) * (batch_size, K, dim) = (batch_size, dim)\n",
    "\n",
    "neg:\n",
    "            caps                     y                  weights\n",
    "    (batch_size, K, dim) * (batch_size, dim, nNeg) = (batch_size, K, nNeg)\n",
    "\n",
    "            weights                caps\n",
    "    (batch_size, nNeg, K) * (batch_size, K, dim) = (batch_size, nNeg, dim)\n",
    "\n",
    "\"\"\"\n",
    "his = th.matmul(\n",
    "    th.softmax(\n",
    "        th.pow(th.transpose(th.matmul(caps, tar.view(bs, D, 1)), 1, 2), 2),\n",
    "        dim=2\n",
    "    ), \n",
    "    caps\n",
    ").view(bs, D) # (batch_size, dim)\n",
    "\n",
    "# pos logits\n",
    "tmp = 0.01\n",
    "his = his / th.norm(his, dim=1).view(bs, 1)\n",
    "tar = tar / th.norm(tar, dim=1).view(bs, 1)\n",
    "posLogits = th.sigmoid(th.sum(his * tar, dim=1) / tmp)\n",
    "\n",
    "# neg logits\n",
    "nNeg = 5\n",
    "tarNeg = tar[th.multinomial(th.ones(bs), nNeg * bs, replacement=True)].view(bs, nNeg, D) # (batch_size, nNeg, D)\n",
    "yNegT = th.transpose(tar[th.multinomial(th.ones(bs), nNeg * bs, replacement=True)].view(bs, nNeg, D), 1, 2) # (batch_size, D, nNeg)\n",
    "hisNeg = th.matmul(\n",
    "    th.softmax(\n",
    "        th.pow(th.transpose(th.matmul(caps, yNegT), 1, 2), 2),\n",
    "        dim=2\n",
    "    ),  # (batch_size, nNeg, K)\n",
    "    caps\n",
    ") # (batch_size, nNeg, D)\n",
    "# hisNeg[b][i].dot(tarNeg[b][i]) for all b, i\n",
    "negLogits = th.sigmoid(th.sum(hisNeg * tarNeg, dim=2).view(bs * nNeg) / tmp)\n",
    "\n",
    "logits = th.concat([posLogits, negLogits])\n",
    "labels = th.concat([th.ones(bs, ), th.zeros(bs * nNeg)])\n",
    "\n",
    "# loss\n",
    "CELoss = th.nn.BCELoss()\n",
    "loss = CELoss(logits, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "wij * Se\n",
    "\n",
    "wji is more convenient\n",
    "\n",
    "[w00, w01, w02] * each sample seq -> cap0\n",
    "[w10, w11, w12] * each sample seq -> cap1\n",
    "[w20, w21, w22] * each sample seq -> cap2\n",
    "\n",
    "\n",
    "[[w00, w01, w02]                            \n",
    " [w10, w11, w12]    *  each sample seq -> (k, dim)\n",
    " [w20, w21, w22]]\n",
    "\"\"\"\n",
    "w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7815ee787aebb751c33c27d6a55d2dad08e47ba75cac09f21612304d6f659967"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
