{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_USER_FREQ = 20\n",
    "MIN_ITEM_FREQ = 100\n",
    "\n",
    "#ratings = pd.read_csv(\"data/Books.csv\", header=None, nrows=500000)\n",
    "ratings = pd.read_csv(\"data/Books.csv\", header=None)\n",
    "ratings.columns = ['userId', 'itemId', 'rate', 'timestamp']\n",
    "\n",
    "# user filtering\n",
    "userFreq = ratings.groupby(['userId'])['userId'].count()\n",
    "validSet = set(userFreq.loc[userFreq >= MIN_USER_FREQ].index)\n",
    "ratings = ratings.loc[ratings['userId'].apply(lambda x: x in validSet), :]\n",
    "\n",
    "# item filtering\n",
    "itemFreq = ratings.groupby(['itemId'])['itemId'].count()\n",
    "validSet = set(itemFreq.loc[itemFreq >= MIN_ITEM_FREQ].index)\n",
    "ratings = ratings.loc[ratings['itemId'].apply(lambda x: x in validSet), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode his\n",
    "ukv, ikv = list(enumerate(ratings['userId'].unique())), list(enumerate(ratings['itemId'].unique()))\n",
    "userRawId = {encId: rawId for encId, rawId in ukv}\n",
    "userEncId = {rawId: encId for encId, rawId in ukv}\n",
    "\n",
    "# encode tar, id 0 is for padding, item encode id start from 1\n",
    "itemRawId = {encId + 1: rawId for encId, rawId in ikv}\n",
    "itemEncId = {rawId: encId + 1 for encId, rawId in ikv}\n",
    "\n",
    "# 编码\n",
    "ratings['userId'] = ratings['userId'].apply(lambda x: userEncId[x])\n",
    "ratings['itemId'] = ratings['itemId'].apply(lambda x: itemEncId[x])\n",
    "\n",
    "ratings.sort_values(by=['userId', 'timestamp'], inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "winSize = 15\n",
    "\n",
    "def padOrCut(seq, l):\n",
    "    if (len(seq) < l): return np.concatenate([seq, (l - len(seq)) * [0]])\n",
    "    elif (len(seq) > l): return seq[:l]\n",
    "    else: return seq\n",
    "\n",
    "# generate user sample by sliding window\n",
    "def genUserSamples(userDf):\n",
    "    userDf.reset_index(drop=True, inplace=True)\n",
    "    his, tar = [], []\n",
    "    for i in range(1, userDf.shape[0]): # enumerate y from 1\n",
    "        # x = window [i - winSize, i - 1], y = item[i]\n",
    "        his.append(padOrCut(userDf.iloc[max(0, i - winSize):i]['itemId'].values, winSize))\n",
    "        tar.append(userDf.iloc[i]['itemId'])\n",
    "    # split train and test as 9:1\n",
    "    i = int(len(his) * 0.9)\n",
    "    trainHis, testHis = his[:i], his[i:]\n",
    "    trainTar, testTar = tar[:i], tar[i:]\n",
    "\n",
    "    return (np.stack(trainHis), np.stack(trainTar)), (np.stack(testHis), np.stack(testTar))\n",
    "\n",
    "samples = ratings.groupby('userId') \\\n",
    "    .filter(lambda x: x.shape[0] >= 10) \\\n",
    "    .groupby('userId') \\\n",
    "    .apply(genUserSamples)\n",
    "\n",
    "trainHis = samples.apply(lambda x: x[0][0]).values\n",
    "trainTar = samples.apply(lambda x: x[0][1]).values\n",
    "testHis = samples.apply(lambda x: x[1][0]).values\n",
    "testTar = samples.apply(lambda x: x[1][1]).values\n",
    "\n",
    "assert len(trainHis) > 0\n",
    "trainHis = np.concatenate(trainHis).astype(np.int32)\n",
    "trainTar = np.concatenate(trainTar).astype(np.int32)\n",
    "testHis = np.concatenate(testHis).astype(np.int32)\n",
    "testTar = np.concatenate(testTar).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, his, tar):\n",
    "        self.his = his\n",
    "        self.tar = tar\n",
    "        assert self.his.shape[0] == self.tar.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.his[i], self.tar[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.his.shape[0]\n",
    "\n",
    "trainData = DataLoader(\n",
    "    Dataset(trainHis, trainTar),\n",
    "    batch_size = 1028,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIND(th.nn.Module):\n",
    "    def __init__(self, D, K, R, L, nNeg, embedNum):\n",
    "        super(MIND, self).__init__()\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "        self.R = R\n",
    "        self.L = L\n",
    "        self.nNeg = nNeg\n",
    "        # weights initialization\n",
    "        self.itemEmbeds = th.nn.Embedding(embedNum, D, padding_idx=0)\n",
    "        # matmul([batch_size, k, 1, dim], [k, dim, dim']) = [batch_size, k, 1, dim']\n",
    "        self.dense1 = th.empty(K, D, 2 * D)\n",
    "        self.dense2 = th.empty(K, 2 * D, D)\n",
    "        # one S for all routing operations, first dim is for batch broadcasting\n",
    "        self.S = th.empty(D, D)\n",
    "        th.nn.init.normal_(self.S, mean=0.0, std=1.0)\n",
    "        self.dense1 = th.nn.init.normal_(self.dense1, mean=0.0, std=1.0)\n",
    "        self.dense2 = th.nn.init.normal_(self.dense2, mean=0.0, std=1.0)\n",
    "        self.opt = th.optim.Adam(self.parameters(), lr=0.05)\n",
    "\n",
    "    # output caps' length is in (0, 1)\n",
    "    def squash(self, caps, bs):\n",
    "        n = th.norm(caps, dim=2).view(bs, self.K, 1)\n",
    "        nSquare = th.pow(n, 2)\n",
    "\n",
    "        return (nSquare / ((1 + nSquare) * n + 1e-9)) * caps\n",
    "    \n",
    "    def B2IRouting(self, his, bs):\n",
    "        # B2I dynamic routing, input behaviors, output caps\n",
    "        # init b, bji = b[j][i] no grad for b: https://github.com/Ugenteraan/CapsNet-PyTorch/blob/master/CapsNet-PyTorch.ipynb\n",
    "        b = th.normal(0, 1, (self.K, self.L)).detach()\n",
    "        # k is fixed for batch forward, cannot find a way to use variant k with batch\n",
    "        I = self.itemEmbeds(his) # (batch_size, len, dim)\n",
    "        # bilinear transform & l2 norm, Sei is fixde during routing \n",
    "        I = th.matmul(I, self.S) # (batch_size, L, dim)\n",
    "        I = I / (th.norm(I, dim=2) + 1e-9).view(bs, self.L, 1)\n",
    "        for i in range(self.R):\n",
    "            # routing, cut w's gradient, because caps have gradients from w and w is changing over loops\n",
    "            w = th.softmax(b, dim=1).detach() # (K, L)\n",
    "            if i != self.R - 1:\n",
    "                # no grads\n",
    "                with th.no_grad():\n",
    "                    caps = self.squash(th.matmul(w, I), bs) # (batch_size, K, dim)\n",
    "                    # update routing logits\n",
    "                    _b = th.matmul(caps, th.transpose(I, 1, 2)) # (batch_size, K, L), _bji = _b[j][i]\n",
    "                    # sum over batch dim first, then add to b\n",
    "                    b += th.sum(_b, dim=0) # (K, L)\n",
    "            else:\n",
    "                caps = self.squash(th.matmul(w, I), bs) # (batch_size, K, dim)\n",
    "                # skip routing logits update in last round\n",
    "        # mlp\n",
    "        caps = th.matmul(caps.view(bs, self.K, 1, self.D), self.dense1)\n",
    "        caps = th.matmul(caps, self.dense2).view(bs, self.K, self.D)\n",
    "        # l2 norm\n",
    "        caps = caps / (th.norm(caps, dim=2).view(bs, self.K, 1) + 1e-9)\n",
    "        \n",
    "        return caps\n",
    "    \n",
    "    def labelAwareAttation(self, caps, tar, bs, p=2):\n",
    "        # label-aware attention, input caps and targets, output logits\n",
    "        tar = self.itemEmbeds(tar) # (batch_size, dim)\n",
    "        # in-batch negative sampling\n",
    "        his = th.matmul(\n",
    "            th.softmax(\n",
    "                th.pow(th.transpose(th.matmul(caps, tar.view(bs, self.D, 1)), 1, 2), p),\n",
    "                dim=2\n",
    "            ), \n",
    "            caps\n",
    "        ).view(bs, self.D) # (batch_size, dim)\n",
    "\n",
    "        # pos logits\n",
    "        tmp = 0.01\n",
    "        his = his / (th.norm(his, dim=1).view(bs, 1) + 1e-9)\n",
    "        tar = tar / (th.norm(tar, dim=1).view(bs, 1) + 1e-9)\n",
    "        posLogits = th.sigmoid(th.sum(his * tar, dim=1) / tmp)\n",
    "\n",
    "        # neg logits\n",
    "        tarNeg = tar[th.multinomial(th.ones(bs), self.nNeg * bs, replacement=True)].view(bs, self.nNeg, self.D) # (batch_size, nNeg, D)\n",
    "        tarNegT = th.transpose(tarNeg, 1, 2) # (batch_size, D, nNeg)\n",
    "        hisNeg = th.matmul(\n",
    "            th.softmax(\n",
    "                th.pow(th.transpose(th.matmul(caps, tarNegT), 1, 2), p),\n",
    "                dim=2\n",
    "            ),  # (batch_size, nNeg, K)\n",
    "            caps\n",
    "        ) # (batch_size, nNeg, D)\n",
    "        # hisNeg[b][i].dot(tarNeg[b][i]) for all b, i\n",
    "        negLogits = th.sigmoid(th.sum(hisNeg * tarNeg, dim=2).view(bs * self.nNeg) / tmp)\n",
    "\n",
    "        logits = th.concat([posLogits, negLogits])\n",
    "        labels = th.concat([th.ones(bs, ), th.zeros(bs * self.nNeg)])\n",
    "\n",
    "        return logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Step 00000 | Loss 21.873215\n",
      "Epoch 00 | Step 00200 | Loss 20.231533\n",
      "Epoch 00 | Step 00400 | Loss 18.824614\n",
      "Epoch 00 | Step 00600 | Loss 17.532839\n",
      "Epoch 00 | Step 00800 | Loss 16.270983\n",
      "Epoch 00 | Step 01000 | Loss 15.079107\n",
      "Epoch 00 | Step 01200 | Loss 13.975615\n",
      "Epoch 00 | Step 01400 | Loss 12.952223\n",
      "Epoch 00 | Step 01600 | Loss 12.010619\n",
      "Epoch 00 | Step 01800 | Loss 11.153078\n",
      "Epoch 00 | Step 02000 | Loss 10.388562\n",
      "Epoch 00 | Step 02200 | Loss 9.704868\n",
      "Epoch 00 | Step 02400 | Loss 9.096917\n",
      "Epoch 00 | Step 02600 | Loss 8.560026\n",
      "Epoch 00 | Step 02800 | Loss 8.081849\n",
      "Epoch 00 | Step 03000 | Loss 7.653366\n",
      "Epoch 01 | Step 00000 | Loss 1.504871\n",
      "Epoch 01 | Step 00200 | Loss 1.428777\n",
      "Epoch 01 | Step 00400 | Loss 1.387291\n",
      "Epoch 01 | Step 00600 | Loss 1.353326\n",
      "Epoch 01 | Step 00800 | Loss 1.325077\n",
      "Epoch 01 | Step 01000 | Loss 1.298531\n",
      "Epoch 01 | Step 01200 | Loss 1.277683\n",
      "Epoch 01 | Step 01400 | Loss 1.257927\n",
      "Epoch 01 | Step 01600 | Loss 1.241712\n",
      "Epoch 01 | Step 01800 | Loss 1.225214\n",
      "Epoch 01 | Step 02000 | Loss 1.210712\n",
      "Epoch 01 | Step 02200 | Loss 1.197890\n",
      "Epoch 01 | Step 02400 | Loss 1.186345\n",
      "Epoch 01 | Step 02600 | Loss 1.175152\n",
      "Epoch 01 | Step 02800 | Loss 1.165153\n",
      "Epoch 01 | Step 03000 | Loss 1.155676\n",
      "Epoch 02 | Step 00000 | Loss 0.995833\n",
      "Epoch 02 | Step 00200 | Loss 1.012859\n",
      "Epoch 02 | Step 00400 | Loss 1.007141\n",
      "Epoch 02 | Step 00600 | Loss 1.002460\n",
      "Epoch 02 | Step 00800 | Loss 0.999337\n",
      "Epoch 02 | Step 01000 | Loss 0.995975\n",
      "Epoch 02 | Step 01200 | Loss 0.992435\n",
      "Epoch 02 | Step 01400 | Loss 0.989187\n",
      "Epoch 02 | Step 01600 | Loss 0.986364\n",
      "Epoch 02 | Step 01800 | Loss 0.983547\n",
      "Epoch 02 | Step 02000 | Loss 0.980773\n",
      "Epoch 02 | Step 02200 | Loss 0.977923\n",
      "Epoch 02 | Step 02400 | Loss 0.975142\n",
      "Epoch 02 | Step 02600 | Loss 0.972351\n",
      "Epoch 02 | Step 02800 | Loss 0.969755\n",
      "Epoch 02 | Step 03000 | Loss 0.967124\n",
      "Epoch 03 | Step 00000 | Loss 0.889067\n",
      "Epoch 03 | Step 00200 | Loss 0.920578\n",
      "Epoch 03 | Step 00400 | Loss 0.918159\n",
      "Epoch 03 | Step 00600 | Loss 0.915905\n",
      "Epoch 03 | Step 00800 | Loss 0.914328\n",
      "Epoch 03 | Step 01000 | Loss 0.912614\n",
      "Epoch 03 | Step 01200 | Loss 0.910684\n",
      "Epoch 03 | Step 01400 | Loss 0.908812\n",
      "Epoch 03 | Step 01600 | Loss 0.907213\n",
      "Epoch 03 | Step 01800 | Loss 0.905925\n",
      "Epoch 03 | Step 02000 | Loss 0.904006\n",
      "Epoch 03 | Step 02200 | Loss 0.902238\n",
      "Epoch 03 | Step 02400 | Loss 0.900721\n",
      "Epoch 03 | Step 02600 | Loss 0.898966\n",
      "Epoch 03 | Step 02800 | Loss 0.897174\n",
      "Epoch 03 | Step 03000 | Loss 0.895622\n",
      "Epoch 04 | Step 00000 | Loss 0.879227\n",
      "Epoch 04 | Step 00200 | Loss 0.867465\n",
      "Epoch 04 | Step 00400 | Loss 0.866088\n",
      "Epoch 04 | Step 00600 | Loss 0.865604\n",
      "Epoch 04 | Step 00800 | Loss 0.864173\n",
      "Epoch 04 | Step 01000 | Loss 0.862548\n",
      "Epoch 04 | Step 01200 | Loss 0.861042\n",
      "Epoch 04 | Step 01400 | Loss 0.859798\n",
      "Epoch 04 | Step 01600 | Loss 0.858519\n",
      "Epoch 04 | Step 01800 | Loss 0.857491\n",
      "Epoch 04 | Step 02000 | Loss 0.856178\n",
      "Epoch 04 | Step 02200 | Loss 0.854914\n",
      "Epoch 04 | Step 02400 | Loss 0.853830\n",
      "Epoch 04 | Step 02600 | Loss 0.852846\n",
      "Epoch 04 | Step 02800 | Loss 0.851675\n",
      "Epoch 04 | Step 03000 | Loss 0.850479\n",
      "Epoch 05 | Step 00000 | Loss 0.841909\n",
      "Epoch 05 | Step 00200 | Loss 0.829463\n",
      "Epoch 05 | Step 00400 | Loss 0.828487\n",
      "Epoch 05 | Step 00600 | Loss 0.827091\n",
      "Epoch 05 | Step 00800 | Loss 0.826861\n",
      "Epoch 05 | Step 01000 | Loss 0.826338\n",
      "Epoch 05 | Step 01200 | Loss 0.825417\n",
      "Epoch 05 | Step 01400 | Loss 0.824641\n",
      "Epoch 05 | Step 01600 | Loss 0.824015\n",
      "Epoch 05 | Step 01800 | Loss 0.823142\n",
      "Epoch 05 | Step 02000 | Loss 0.822212\n",
      "Epoch 05 | Step 02200 | Loss 0.821426\n",
      "Epoch 05 | Step 02400 | Loss 0.820401\n",
      "Epoch 05 | Step 02600 | Loss 0.819554\n",
      "Epoch 05 | Step 02800 | Loss 0.818676\n",
      "Epoch 05 | Step 03000 | Loss 0.817719\n",
      "Epoch 06 | Step 00000 | Loss 0.808832\n",
      "Epoch 06 | Step 00200 | Loss 0.802497\n",
      "Epoch 06 | Step 00400 | Loss 0.801493\n",
      "Epoch 06 | Step 00600 | Loss 0.800433\n",
      "Epoch 06 | Step 00800 | Loss 0.799906\n",
      "Epoch 06 | Step 01000 | Loss 0.799149\n",
      "Epoch 06 | Step 01200 | Loss 0.798284\n",
      "Epoch 06 | Step 01400 | Loss 0.797513\n",
      "Epoch 06 | Step 01600 | Loss 0.796503\n",
      "Epoch 06 | Step 01800 | Loss 0.795799\n",
      "Epoch 06 | Step 02000 | Loss 0.795200\n",
      "Epoch 06 | Step 02200 | Loss 0.794489\n",
      "Epoch 06 | Step 02400 | Loss 0.793602\n",
      "Epoch 06 | Step 02600 | Loss 0.792975\n",
      "Epoch 06 | Step 02800 | Loss 0.792105\n",
      "Epoch 06 | Step 03000 | Loss 0.791373\n",
      "Epoch 07 | Step 00000 | Loss 0.818422\n",
      "Epoch 07 | Step 00200 | Loss 0.778439\n",
      "Epoch 07 | Step 00400 | Loss 0.778485\n",
      "Epoch 07 | Step 00600 | Loss 0.778035\n",
      "Epoch 07 | Step 00800 | Loss 0.777107\n",
      "Epoch 07 | Step 01000 | Loss 0.775921\n",
      "Epoch 07 | Step 01200 | Loss 0.775657\n",
      "Epoch 07 | Step 01400 | Loss 0.774845\n",
      "Epoch 07 | Step 01600 | Loss 0.774081\n",
      "Epoch 07 | Step 01800 | Loss 0.773393\n",
      "Epoch 07 | Step 02000 | Loss 0.772690\n",
      "Epoch 07 | Step 02200 | Loss 0.771957\n",
      "Epoch 07 | Step 02400 | Loss 0.771253\n",
      "Epoch 07 | Step 02600 | Loss 0.770515\n",
      "Epoch 07 | Step 02800 | Loss 0.769849\n",
      "Epoch 07 | Step 03000 | Loss 0.769302\n",
      "Epoch 08 | Step 00000 | Loss 0.767004\n",
      "Epoch 08 | Step 00200 | Loss 0.756823\n",
      "Epoch 08 | Step 00400 | Loss 0.756748\n",
      "Epoch 08 | Step 00600 | Loss 0.755776\n",
      "Epoch 08 | Step 00800 | Loss 0.754676\n",
      "Epoch 08 | Step 01000 | Loss 0.754411\n",
      "Epoch 08 | Step 01200 | Loss 0.753907\n",
      "Epoch 08 | Step 01400 | Loss 0.753483\n",
      "Epoch 08 | Step 01600 | Loss 0.753170\n",
      "Epoch 08 | Step 01800 | Loss 0.752520\n",
      "Epoch 08 | Step 02000 | Loss 0.751984\n",
      "Epoch 08 | Step 02200 | Loss 0.751261\n",
      "Epoch 08 | Step 02400 | Loss 0.750677\n",
      "Epoch 08 | Step 02600 | Loss 0.750006\n",
      "Epoch 08 | Step 02800 | Loss 0.749365\n",
      "Epoch 08 | Step 03000 | Loss 0.748886\n",
      "Epoch 09 | Step 00000 | Loss 0.738512\n",
      "Epoch 09 | Step 00200 | Loss 0.739714\n",
      "Epoch 09 | Step 00400 | Loss 0.738104\n",
      "Epoch 09 | Step 00600 | Loss 0.737237\n",
      "Epoch 09 | Step 00800 | Loss 0.737165\n",
      "Epoch 09 | Step 01000 | Loss 0.736857\n",
      "Epoch 09 | Step 01200 | Loss 0.736165\n",
      "Epoch 09 | Step 01400 | Loss 0.735464\n",
      "Epoch 09 | Step 01600 | Loss 0.734716\n",
      "Epoch 09 | Step 01800 | Loss 0.734037\n",
      "Epoch 09 | Step 02000 | Loss 0.733463\n",
      "Epoch 09 | Step 02200 | Loss 0.732750\n",
      "Epoch 09 | Step 02400 | Loss 0.731988\n",
      "Epoch 09 | Step 02600 | Loss 0.731305\n",
      "Epoch 09 | Step 02800 | Loss 0.730699\n",
      "Epoch 09 | Step 03000 | Loss 0.730019\n",
      "Epoch 10 | Step 00000 | Loss 0.712479\n",
      "Epoch 10 | Step 00200 | Loss 0.720282\n",
      "Epoch 10 | Step 00400 | Loss 0.719893\n",
      "Epoch 10 | Step 00600 | Loss 0.719187\n",
      "Epoch 10 | Step 00800 | Loss 0.719284\n",
      "Epoch 10 | Step 01000 | Loss 0.718552\n",
      "Epoch 10 | Step 01200 | Loss 0.717777\n",
      "Epoch 10 | Step 01400 | Loss 0.716806\n",
      "Epoch 10 | Step 01600 | Loss 0.716240\n",
      "Epoch 10 | Step 01800 | Loss 0.715897\n",
      "Epoch 10 | Step 02000 | Loss 0.715309\n",
      "Epoch 10 | Step 02200 | Loss 0.714645\n",
      "Epoch 10 | Step 02400 | Loss 0.713987\n",
      "Epoch 10 | Step 02600 | Loss 0.713331\n",
      "Epoch 10 | Step 02800 | Loss 0.712788\n",
      "Epoch 10 | Step 03000 | Loss 0.712256\n",
      "Epoch 11 | Step 00000 | Loss 0.690819\n",
      "Epoch 11 | Step 00200 | Loss 0.701862\n",
      "Epoch 11 | Step 00400 | Loss 0.702117\n",
      "Epoch 11 | Step 00600 | Loss 0.701725\n",
      "Epoch 11 | Step 00800 | Loss 0.701200\n",
      "Epoch 11 | Step 01000 | Loss 0.700849\n",
      "Epoch 11 | Step 01200 | Loss 0.700542\n",
      "Epoch 11 | Step 01400 | Loss 0.700163\n",
      "Epoch 11 | Step 01600 | Loss 0.699650\n",
      "Epoch 11 | Step 01800 | Loss 0.699109\n",
      "Epoch 11 | Step 02000 | Loss 0.698531\n",
      "Epoch 11 | Step 02200 | Loss 0.697895\n",
      "Epoch 11 | Step 02400 | Loss 0.697226\n",
      "Epoch 11 | Step 02600 | Loss 0.696794\n",
      "Epoch 11 | Step 02800 | Loss 0.696238\n",
      "Epoch 11 | Step 03000 | Loss 0.695735\n",
      "Epoch 12 | Step 00000 | Loss 0.666847\n",
      "Epoch 12 | Step 00200 | Loss 0.686675\n",
      "Epoch 12 | Step 00400 | Loss 0.685675\n",
      "Epoch 12 | Step 00600 | Loss 0.685405\n",
      "Epoch 12 | Step 00800 | Loss 0.684995\n",
      "Epoch 12 | Step 01000 | Loss 0.684670\n",
      "Epoch 12 | Step 01200 | Loss 0.684391\n",
      "Epoch 12 | Step 01400 | Loss 0.683944\n",
      "Epoch 12 | Step 01600 | Loss 0.683553\n",
      "Epoch 12 | Step 01800 | Loss 0.682998\n",
      "Epoch 12 | Step 02000 | Loss 0.682669\n",
      "Epoch 12 | Step 02200 | Loss 0.682162\n",
      "Epoch 12 | Step 02400 | Loss 0.681673\n",
      "Epoch 12 | Step 02600 | Loss 0.681206\n",
      "Epoch 12 | Step 02800 | Loss 0.680757\n",
      "Epoch 12 | Step 03000 | Loss 0.680332\n",
      "Epoch 13 | Step 00000 | Loss 0.679381\n",
      "Epoch 13 | Step 00200 | Loss 0.672540\n",
      "Epoch 13 | Step 00400 | Loss 0.672619\n",
      "Epoch 13 | Step 00600 | Loss 0.671951\n",
      "Epoch 13 | Step 00800 | Loss 0.671360\n",
      "Epoch 13 | Step 01000 | Loss 0.671251\n",
      "Epoch 13 | Step 01200 | Loss 0.671064\n",
      "Epoch 13 | Step 01400 | Loss 0.670697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Step 01600 | Loss 0.670315\n",
      "Epoch 13 | Step 01800 | Loss 0.669800\n",
      "Epoch 13 | Step 02000 | Loss 0.669225\n",
      "Epoch 13 | Step 02200 | Loss 0.668835\n",
      "Epoch 13 | Step 02400 | Loss 0.668437\n",
      "Epoch 13 | Step 02600 | Loss 0.667910\n",
      "Epoch 13 | Step 02800 | Loss 0.667550\n",
      "Epoch 13 | Step 03000 | Loss 0.667188\n",
      "Epoch 14 | Step 00000 | Loss 0.648145\n",
      "Epoch 14 | Step 00200 | Loss 0.661366\n",
      "Epoch 14 | Step 00400 | Loss 0.661049\n",
      "Epoch 14 | Step 00600 | Loss 0.660568\n",
      "Epoch 14 | Step 00800 | Loss 0.659733\n",
      "Epoch 14 | Step 01000 | Loss 0.659392\n",
      "Epoch 14 | Step 01200 | Loss 0.659010\n",
      "Epoch 14 | Step 01400 | Loss 0.658431\n",
      "Epoch 14 | Step 01600 | Loss 0.658142\n",
      "Epoch 14 | Step 01800 | Loss 0.657884\n",
      "Epoch 14 | Step 02000 | Loss 0.657328\n",
      "Epoch 14 | Step 02200 | Loss 0.656913\n",
      "Epoch 14 | Step 02400 | Loss 0.656462\n",
      "Epoch 14 | Step 02600 | Loss 0.656250\n",
      "Epoch 14 | Step 02800 | Loss 0.655990\n",
      "Epoch 14 | Step 03000 | Loss 0.655708\n",
      "Epoch 15 | Step 00000 | Loss 0.669415\n",
      "Epoch 15 | Step 00200 | Loss 0.650782\n",
      "Epoch 15 | Step 00400 | Loss 0.649728\n",
      "Epoch 15 | Step 00600 | Loss 0.649006\n",
      "Epoch 15 | Step 00800 | Loss 0.648793\n",
      "Epoch 15 | Step 01000 | Loss 0.648265\n",
      "Epoch 15 | Step 01200 | Loss 0.648062\n",
      "Epoch 15 | Step 01400 | Loss 0.647699\n",
      "Epoch 15 | Step 01600 | Loss 0.647268\n",
      "Epoch 15 | Step 01800 | Loss 0.646759\n",
      "Epoch 15 | Step 02000 | Loss 0.646419\n",
      "Epoch 15 | Step 02200 | Loss 0.646405\n",
      "Epoch 15 | Step 02400 | Loss 0.645988\n",
      "Epoch 15 | Step 02600 | Loss 0.645530\n",
      "Epoch 15 | Step 02800 | Loss 0.645238\n",
      "Epoch 15 | Step 03000 | Loss 0.644930\n",
      "Epoch 16 | Step 00000 | Loss 0.615389\n",
      "Epoch 16 | Step 00200 | Loss 0.638627\n",
      "Epoch 16 | Step 00400 | Loss 0.639070\n",
      "Epoch 16 | Step 00600 | Loss 0.638819\n",
      "Epoch 16 | Step 00800 | Loss 0.638388\n",
      "Epoch 16 | Step 01000 | Loss 0.637765\n",
      "Epoch 16 | Step 01200 | Loss 0.637564\n",
      "Epoch 16 | Step 01400 | Loss 0.637432\n",
      "Epoch 16 | Step 01600 | Loss 0.637370\n",
      "Epoch 16 | Step 01800 | Loss 0.637161\n",
      "Epoch 16 | Step 02000 | Loss 0.636912\n",
      "Epoch 16 | Step 02200 | Loss 0.636549\n",
      "Epoch 16 | Step 02400 | Loss 0.636131\n",
      "Epoch 16 | Step 02600 | Loss 0.635827\n",
      "Epoch 16 | Step 02800 | Loss 0.635692\n",
      "Epoch 16 | Step 03000 | Loss 0.635419\n",
      "Epoch 17 | Step 00000 | Loss 0.659188\n",
      "Epoch 17 | Step 00200 | Loss 0.630997\n",
      "Epoch 17 | Step 00400 | Loss 0.631160\n",
      "Epoch 17 | Step 00600 | Loss 0.630606\n",
      "Epoch 17 | Step 00800 | Loss 0.630247\n",
      "Epoch 17 | Step 01000 | Loss 0.629932\n",
      "Epoch 17 | Step 01200 | Loss 0.629888\n",
      "Epoch 17 | Step 01400 | Loss 0.629393\n",
      "Epoch 17 | Step 01600 | Loss 0.629187\n",
      "Epoch 17 | Step 01800 | Loss 0.628815\n",
      "Epoch 17 | Step 02000 | Loss 0.628651\n",
      "Epoch 17 | Step 02200 | Loss 0.628295\n",
      "Epoch 17 | Step 02400 | Loss 0.627953\n",
      "Epoch 17 | Step 02600 | Loss 0.627616\n",
      "Epoch 17 | Step 02800 | Loss 0.627480\n",
      "Epoch 17 | Step 03000 | Loss 0.627099\n",
      "Epoch 18 | Step 00000 | Loss 0.616400\n",
      "Epoch 18 | Step 00200 | Loss 0.621178\n",
      "Epoch 18 | Step 00400 | Loss 0.621401\n",
      "Epoch 18 | Step 00600 | Loss 0.620901\n",
      "Epoch 18 | Step 00800 | Loss 0.621050\n",
      "Epoch 18 | Step 01000 | Loss 0.620900\n",
      "Epoch 18 | Step 01200 | Loss 0.620597\n",
      "Epoch 18 | Step 01400 | Loss 0.620116\n",
      "Epoch 18 | Step 01600 | Loss 0.619891\n",
      "Epoch 18 | Step 01800 | Loss 0.619933\n",
      "Epoch 18 | Step 02000 | Loss 0.619859\n",
      "Epoch 18 | Step 02200 | Loss 0.619535\n",
      "Epoch 18 | Step 02400 | Loss 0.619355\n",
      "Epoch 18 | Step 02600 | Loss 0.619070\n",
      "Epoch 18 | Step 02800 | Loss 0.618812\n",
      "Epoch 18 | Step 03000 | Loss 0.618592\n",
      "Epoch 19 | Step 00000 | Loss 0.612127\n",
      "Epoch 19 | Step 00200 | Loss 0.612339\n",
      "Epoch 19 | Step 00400 | Loss 0.612804\n",
      "Epoch 19 | Step 00600 | Loss 0.613388\n",
      "Epoch 19 | Step 00800 | Loss 0.613550\n",
      "Epoch 19 | Step 01000 | Loss 0.613401\n",
      "Epoch 19 | Step 01200 | Loss 0.613365\n",
      "Epoch 19 | Step 01400 | Loss 0.613437\n",
      "Epoch 19 | Step 01600 | Loss 0.613222\n",
      "Epoch 19 | Step 01800 | Loss 0.612964\n",
      "Epoch 19 | Step 02000 | Loss 0.612733\n",
      "Epoch 19 | Step 02200 | Loss 0.612367\n",
      "Epoch 19 | Step 02400 | Loss 0.612158\n",
      "Epoch 19 | Step 02600 | Loss 0.611859\n",
      "Epoch 19 | Step 02800 | Loss 0.611717\n",
      "Epoch 19 | Step 03000 | Loss 0.611554\n",
      "Epoch 20 | Step 00000 | Loss 0.602841\n",
      "Epoch 20 | Step 00200 | Loss 0.608185\n",
      "Epoch 20 | Step 00400 | Loss 0.607974\n",
      "Epoch 20 | Step 00600 | Loss 0.607283\n",
      "Epoch 20 | Step 00800 | Loss 0.606753\n",
      "Epoch 20 | Step 01000 | Loss 0.606838\n",
      "Epoch 20 | Step 01200 | Loss 0.606665\n",
      "Epoch 20 | Step 01400 | Loss 0.606383\n",
      "Epoch 20 | Step 01600 | Loss 0.606292\n",
      "Epoch 20 | Step 01800 | Loss 0.606140\n",
      "Epoch 20 | Step 02000 | Loss 0.605891\n",
      "Epoch 20 | Step 02200 | Loss 0.605791\n",
      "Epoch 20 | Step 02400 | Loss 0.605545\n",
      "Epoch 20 | Step 02600 | Loss 0.605391\n",
      "Epoch 20 | Step 02800 | Loss 0.605108\n",
      "Epoch 20 | Step 03000 | Loss 0.604883\n",
      "Epoch 21 | Step 00000 | Loss 0.608374\n",
      "Epoch 21 | Step 00200 | Loss 0.602550\n",
      "Epoch 21 | Step 00400 | Loss 0.603215\n",
      "Epoch 21 | Step 00600 | Loss 0.603325\n",
      "Epoch 21 | Step 00800 | Loss 0.603078\n",
      "Epoch 21 | Step 01000 | Loss 0.602515\n",
      "Epoch 21 | Step 01200 | Loss 0.602231\n",
      "Epoch 21 | Step 01400 | Loss 0.601651\n",
      "Epoch 21 | Step 01600 | Loss 0.601266\n",
      "Epoch 21 | Step 01800 | Loss 0.600789\n",
      "Epoch 21 | Step 02000 | Loss 0.600672\n",
      "Epoch 21 | Step 02200 | Loss 0.600579\n",
      "Epoch 21 | Step 02400 | Loss 0.600338\n",
      "Epoch 21 | Step 02600 | Loss 0.600115\n",
      "Epoch 21 | Step 02800 | Loss 0.600041\n",
      "Epoch 21 | Step 03000 | Loss 0.599907\n",
      "Epoch 22 | Step 00000 | Loss 0.587608\n",
      "Epoch 22 | Step 00200 | Loss 0.595145\n",
      "Epoch 22 | Step 00400 | Loss 0.595135\n",
      "Epoch 22 | Step 00600 | Loss 0.595240\n",
      "Epoch 22 | Step 00800 | Loss 0.595402\n",
      "Epoch 22 | Step 01000 | Loss 0.595576\n",
      "Epoch 22 | Step 01200 | Loss 0.595628\n",
      "Epoch 22 | Step 01400 | Loss 0.595249\n",
      "Epoch 22 | Step 01600 | Loss 0.595031\n",
      "Epoch 22 | Step 01800 | Loss 0.594673\n",
      "Epoch 22 | Step 02000 | Loss 0.594343\n",
      "Epoch 22 | Step 02200 | Loss 0.594120\n",
      "Epoch 22 | Step 02400 | Loss 0.593925\n",
      "Epoch 22 | Step 02600 | Loss 0.593750\n",
      "Epoch 22 | Step 02800 | Loss 0.593537\n",
      "Epoch 22 | Step 03000 | Loss 0.593488\n",
      "Epoch 23 | Step 00000 | Loss 0.565320\n",
      "Epoch 23 | Step 00200 | Loss 0.591168\n",
      "Epoch 23 | Step 00400 | Loss 0.591014\n",
      "Epoch 23 | Step 00600 | Loss 0.590779\n",
      "Epoch 23 | Step 00800 | Loss 0.591230\n",
      "Epoch 23 | Step 01000 | Loss 0.591049\n",
      "Epoch 23 | Step 01200 | Loss 0.590672\n",
      "Epoch 23 | Step 01400 | Loss 0.590388\n",
      "Epoch 23 | Step 01600 | Loss 0.590258\n",
      "Epoch 23 | Step 01800 | Loss 0.590189\n",
      "Epoch 23 | Step 02000 | Loss 0.589985\n",
      "Epoch 23 | Step 02200 | Loss 0.589893\n",
      "Epoch 23 | Step 02400 | Loss 0.589676\n",
      "Epoch 23 | Step 02600 | Loss 0.589559\n",
      "Epoch 23 | Step 02800 | Loss 0.589284\n",
      "Epoch 23 | Step 03000 | Loss 0.589145\n",
      "Epoch 24 | Step 00000 | Loss 0.591567\n",
      "Epoch 24 | Step 00200 | Loss 0.585494\n",
      "Epoch 24 | Step 00400 | Loss 0.585883\n",
      "Epoch 24 | Step 00600 | Loss 0.585766\n",
      "Epoch 24 | Step 00800 | Loss 0.585462\n",
      "Epoch 24 | Step 01000 | Loss 0.585137\n",
      "Epoch 24 | Step 01200 | Loss 0.585077\n",
      "Epoch 24 | Step 01400 | Loss 0.585008\n",
      "Epoch 24 | Step 01600 | Loss 0.584618\n",
      "Epoch 24 | Step 01800 | Loss 0.584482\n",
      "Epoch 24 | Step 02000 | Loss 0.584257\n",
      "Epoch 24 | Step 02200 | Loss 0.584080\n",
      "Epoch 24 | Step 02400 | Loss 0.583996\n",
      "Epoch 24 | Step 02600 | Loss 0.583807\n",
      "Epoch 24 | Step 02800 | Loss 0.583584\n",
      "Epoch 24 | Step 03000 | Loss 0.583439\n",
      "Epoch 25 | Step 00000 | Loss 0.557835\n",
      "Epoch 25 | Step 00200 | Loss 0.582171\n",
      "Epoch 25 | Step 00400 | Loss 0.582852\n",
      "Epoch 25 | Step 00600 | Loss 0.582570\n",
      "Epoch 25 | Step 00800 | Loss 0.581696\n",
      "Epoch 25 | Step 01000 | Loss 0.581576\n",
      "Epoch 25 | Step 01200 | Loss 0.581361\n",
      "Epoch 25 | Step 01400 | Loss 0.580891\n",
      "Epoch 25 | Step 01600 | Loss 0.580709\n",
      "Epoch 25 | Step 01800 | Loss 0.580515\n",
      "Epoch 25 | Step 02000 | Loss 0.580418\n",
      "Epoch 25 | Step 02200 | Loss 0.580325\n",
      "Epoch 25 | Step 02400 | Loss 0.580078\n",
      "Epoch 25 | Step 02600 | Loss 0.579806\n",
      "Epoch 25 | Step 02800 | Loss 0.579577\n",
      "Epoch 25 | Step 03000 | Loss 0.579360\n",
      "Epoch 26 | Step 00000 | Loss 0.583527\n",
      "Epoch 26 | Step 00200 | Loss 0.577813\n",
      "Epoch 26 | Step 00400 | Loss 0.577035\n",
      "Epoch 26 | Step 00600 | Loss 0.577690\n",
      "Epoch 26 | Step 00800 | Loss 0.577112\n",
      "Epoch 26 | Step 01000 | Loss 0.577008\n",
      "Epoch 26 | Step 01200 | Loss 0.576788\n",
      "Epoch 26 | Step 01400 | Loss 0.576603\n",
      "Epoch 26 | Step 01600 | Loss 0.576964\n",
      "Epoch 26 | Step 01800 | Loss 0.576831\n",
      "Epoch 26 | Step 02000 | Loss 0.576668\n",
      "Epoch 26 | Step 02200 | Loss 0.576346\n",
      "Epoch 26 | Step 02400 | Loss 0.576242\n",
      "Epoch 26 | Step 02600 | Loss 0.576031\n",
      "Epoch 26 | Step 02800 | Loss 0.575908\n",
      "Epoch 26 | Step 03000 | Loss 0.575685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | Step 00000 | Loss 0.570636\n",
      "Epoch 27 | Step 00200 | Loss 0.572867\n",
      "Epoch 27 | Step 00400 | Loss 0.572551\n",
      "Epoch 27 | Step 00600 | Loss 0.572387\n",
      "Epoch 27 | Step 00800 | Loss 0.572314\n",
      "Epoch 27 | Step 01000 | Loss 0.572428\n",
      "Epoch 27 | Step 01200 | Loss 0.572437\n",
      "Epoch 27 | Step 01400 | Loss 0.572343\n",
      "Epoch 27 | Step 01600 | Loss 0.572245\n",
      "Epoch 27 | Step 01800 | Loss 0.571855\n",
      "Epoch 27 | Step 02000 | Loss 0.571794\n",
      "Epoch 27 | Step 02200 | Loss 0.571854\n",
      "Epoch 27 | Step 02400 | Loss 0.571801\n",
      "Epoch 27 | Step 02600 | Loss 0.571634\n",
      "Epoch 27 | Step 02800 | Loss 0.571454\n",
      "Epoch 27 | Step 03000 | Loss 0.571522\n",
      "Epoch 28 | Step 00000 | Loss 0.560554\n",
      "Epoch 28 | Step 00200 | Loss 0.566919\n",
      "Epoch 28 | Step 00400 | Loss 0.567684\n",
      "Epoch 28 | Step 00600 | Loss 0.568508\n",
      "Epoch 28 | Step 00800 | Loss 0.569248\n",
      "Epoch 28 | Step 01000 | Loss 0.569279\n",
      "Epoch 28 | Step 01200 | Loss 0.569365\n",
      "Epoch 28 | Step 01400 | Loss 0.569589\n",
      "Epoch 28 | Step 01600 | Loss 0.569080\n",
      "Epoch 28 | Step 01800 | Loss 0.568967\n",
      "Epoch 28 | Step 02000 | Loss 0.569065\n",
      "Epoch 28 | Step 02200 | Loss 0.568793\n",
      "Epoch 28 | Step 02400 | Loss 0.568763\n",
      "Epoch 28 | Step 02600 | Loss 0.568818\n",
      "Epoch 28 | Step 02800 | Loss 0.568628\n",
      "Epoch 28 | Step 03000 | Loss 0.568440\n",
      "Epoch 29 | Step 00000 | Loss 0.520630\n",
      "Epoch 29 | Step 00200 | Loss 0.566961\n",
      "Epoch 29 | Step 00400 | Loss 0.567188\n",
      "Epoch 29 | Step 00600 | Loss 0.566706\n",
      "Epoch 29 | Step 00800 | Loss 0.566923\n",
      "Epoch 29 | Step 01000 | Loss 0.566584\n",
      "Epoch 29 | Step 01200 | Loss 0.566141\n",
      "Epoch 29 | Step 01400 | Loss 0.565983\n",
      "Epoch 29 | Step 01600 | Loss 0.566134\n",
      "Epoch 29 | Step 01800 | Loss 0.566119\n",
      "Epoch 29 | Step 02000 | Loss 0.565942\n",
      "Epoch 29 | Step 02200 | Loss 0.565928\n",
      "Epoch 29 | Step 02400 | Loss 0.565811\n",
      "Epoch 29 | Step 02600 | Loss 0.565387\n",
      "Epoch 29 | Step 02800 | Loss 0.565073\n",
      "Epoch 29 | Step 03000 | Loss 0.565038\n"
     ]
    }
   ],
   "source": [
    "model = MIND(D=8, K=3, R=3, L=winSize, nNeg=5, embedNum=len(itemEncId) + 1)\n",
    "BCELoss = th.nn.BCELoss()\n",
    "for epoch in range(30):\n",
    "    epochTotalLoss = 0\n",
    "    for step, (his, tar) in enumerate(trainData):\n",
    "        bs = his.shape[0]\n",
    "        caps = model.B2IRouting(his, bs)\n",
    "        logits, labels = model.labelAwareAttation(caps, tar, bs)\n",
    "\n",
    "        loss = BCELoss(logits, labels)\n",
    "        loss.backward()\n",
    "        model.opt.step()\n",
    "        model.opt.zero_grad()\n",
    "        epochTotalLoss += loss\n",
    "        if (step % 200 == 0):\n",
    "            print('Epoch {:02d} | Step {:05d} | Loss {:.6f}'.format(\n",
    "                epoch,\n",
    "                step,\n",
    "                epochTotalLoss / (step + 1),\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@90: 0.0058033789204125245\n"
     ]
    }
   ],
   "source": [
    "testData = DataLoader(\n",
    "    Dataset(testHis, testTar),\n",
    "    batch_size = 8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "with th.no_grad():\n",
    "    ie = model.itemEmbeds.weight\n",
    "    ie /= th.norm(ie, dim=1).view(ie.shape[0], 1) + 1e-9\n",
    "    n, top = ie.shape[0], 30\n",
    "    hit, total = 0, 0\n",
    "    for his, tar in testData:\n",
    "        bs = his.shape[0]\n",
    "        caps = model.B2IRouting(his, bs)\n",
    "        # TODO: should change to label aware attention\n",
    "        logits = th.matmul(caps, th.transpose(ie, 0, 1)).detach().numpy()\n",
    "\n",
    "        # index就是encId, 对第三维进行快速选择，第kth小元素(从0开始)的原始索引值将位于其排序后的最终位置，其他元素小于他的在左，大于他的在右\n",
    "        res = np.argpartition(logits, kth=n - top, axis=2)[:, :, -top:]\n",
    "\n",
    "        for r, truth in zip(res, tar):\n",
    "            # 合并K个兴趣的召回结果\n",
    "            r = set(r.flatten())\n",
    "            if (truth.item() in r): hit += 1\n",
    "            total += 1\n",
    "\n",
    "    print(\"precision@{}: {}\".format(model.K * top, hit / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1.,  ..., 1., 1., 1.], grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.norm(ie, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型过程实验代码 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "K = 4\n",
    "R = 1\n",
    "L = 3\n",
    "\n",
    "itemEmbeds = th.nn.Embedding(len(itemEncId), D, padding_idx=0)\n",
    "\n",
    "\"\"\"\n",
    "    Get number of interest number using equation (9) in the paper\n",
    "    @x: (batch_size, seq_len), input batch user history item seq\n",
    "    @K: basic interest number\n",
    "\n",
    "    @output: (batch_size, )\n",
    "\"\"\"\n",
    "def getK(x, K):\n",
    "    return th.maximum(th.minimum(th.log2(x.count_nonzero(dim=1)), th.tensor([K])), th.tensor([1])).type(th.int8)\n",
    "\n",
    "\"\"\"\n",
    "    squash function using equation (7) in the paper\n",
    "    @caps: (batch_size, k, dim), interest capsules\n",
    "    \n",
    "    @output: (batch_size, k, dim)\n",
    "\"\"\"\n",
    "def squash(caps):\n",
    "    l2Norm = th.norm(caps, dim=2) # (batch_size, k)\n",
    "    l2NormSqure = th.pow(l2Norm, 2)\n",
    "\n",
    "    return (l2NormSqure / (1 + l2NormSqure)).view(bs, K, 1) * (caps / l2Norm.view(bs, K, 1))\n",
    "\n",
    "# weights initialization, \n",
    "# init b, bji = b[j][i]\n",
    "b = th.empty(K, L)\n",
    "th.nn.init.normal_(b, mean=0.0, std=1.0)\n",
    "# one S for all routing operations, first dim is for batch broadcasting\n",
    "S = th.empty(D, D)\n",
    "th.nn.init.normal_(S, mean=0.0, std=1.0)\n",
    "\n",
    "his = th.tensor([[1, 2, 0], [3, 2, 0]])\n",
    "tar = th.tensor([3, 1])\n",
    "batch_labels = th.tensor([1, 0])\n",
    "\n",
    "# B2I dynamic routing, input behaviors, output caps\n",
    "bs = his.shape[0]\n",
    "# k is fixed for batch forward, cannot find a way to use variant k with batch\n",
    "#k = getK(his, K) \n",
    "I = itemEmbeds(his) # (batch_size, len, dim)\n",
    "for i in range(R):\n",
    "    w = th.softmax(b, dim=1) # (K, L)\n",
    "    I = th.matmul(I, S) # (batch_size, len, dim), bilinear transform\n",
    "    caps = squash(th.matmul(w, I)) # (batch_size, K, dim)\n",
    "    _b = th.matmul(caps, th.transpose(I, 1, 2)) # (batch_size, K, L), _bji = _b[j][i]\n",
    "    # sum over batch dim first, then add to b\n",
    "    b += th.sum(_b, dim=0) # (K, L)\n",
    "\n",
    "# label-aware attention, input caps and targets, output logits\n",
    "tar = itemEmbeds(tar) # (batch_size, dim)\n",
    "# in-batch negative sampling\n",
    "\"\"\"\n",
    "pos:\n",
    "            caps                     y                  weights\n",
    "    (batch_size, K, dim) * (batch_size, dim, 1) = (batch_size, K, 1)\n",
    "\n",
    "            weights                caps\n",
    "    (batch_size, 1, K) * (batch_size, K, dim) = (batch_size, dim)\n",
    "\n",
    "neg:\n",
    "            caps                     y                  weights\n",
    "    (batch_size, K, dim) * (batch_size, dim, nNeg) = (batch_size, K, nNeg)\n",
    "\n",
    "            weights                caps\n",
    "    (batch_size, nNeg, K) * (batch_size, K, dim) = (batch_size, nNeg, dim)\n",
    "\n",
    "\"\"\"\n",
    "his = th.matmul(\n",
    "    th.softmax(\n",
    "        th.pow(th.transpose(th.matmul(caps, tar.view(bs, D, 1)), 1, 2), 2),\n",
    "        dim=2\n",
    "    ), \n",
    "    caps\n",
    ").view(bs, D) # (batch_size, dim)\n",
    "\n",
    "# pos logits\n",
    "tmp = 0.01\n",
    "his = his / th.norm(his, dim=1).view(bs, 1)\n",
    "tar = tar / th.norm(tar, dim=1).view(bs, 1)\n",
    "posLogits = th.sigmoid(th.sum(his * tar, dim=1) / tmp)\n",
    "\n",
    "# neg logits\n",
    "nNeg = 5\n",
    "tarNeg = tar[th.multinomial(th.ones(bs), nNeg * bs, replacement=True)].view(bs, nNeg, D) # (batch_size, nNeg, D)\n",
    "yNegT = th.transpose(tar[th.multinomial(th.ones(bs), nNeg * bs, replacement=True)].view(bs, nNeg, D), 1, 2) # (batch_size, D, nNeg)\n",
    "hisNeg = th.matmul(\n",
    "    th.softmax(\n",
    "        th.pow(th.transpose(th.matmul(caps, yNegT), 1, 2), 2),\n",
    "        dim=2\n",
    "    ),  # (batch_size, nNeg, K)\n",
    "    caps\n",
    ") # (batch_size, nNeg, D)\n",
    "# hisNeg[b][i].dot(tarNeg[b][i]) for all b, i\n",
    "negLogits = th.sigmoid(th.sum(hisNeg * tarNeg, dim=2).view(bs * nNeg) / tmp)\n",
    "\n",
    "logits = th.concat([posLogits, negLogits])\n",
    "labels = th.concat([th.ones(bs, ), th.zeros(bs * nNeg)])\n",
    "\n",
    "# loss\n",
    "CELoss = th.nn.BCELoss()\n",
    "loss = CELoss(logits, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "wij * Se\n",
    "\n",
    "wji is more convenient\n",
    "\n",
    "[w00, w01, w02] * each sample seq -> cap0\n",
    "[w10, w11, w12] * each sample seq -> cap1\n",
    "[w20, w21, w22] * each sample seq -> cap2\n",
    "\n",
    "\n",
    "[[w00, w01, w02]                            \n",
    " [w10, w11, w12]    *  each sample seq -> (k, dim)\n",
    " [w20, w21, w22]]\n",
    "\"\"\"\n",
    "w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7815ee787aebb751c33c27d6a55d2dad08e47ba75cac09f21612304d6f659967"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
