{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_USER_FREQ = 20\n",
    "MIN_ITEM_FREQ = 100\n",
    "\n",
    "ratings = pd.read_csv(\"data/Books.csv\", header=None)\n",
    "ratings.columns = ['userId', 'itemId', 'rate', 'timestamp']\n",
    "\n",
    "# item filtering\n",
    "itemFreq = ratings.groupby(['itemId'])['itemId'].count()\n",
    "validSet = set(itemFreq.loc[itemFreq >= MIN_ITEM_FREQ].index)\n",
    "ratings = ratings.loc[ratings['itemId'].apply(lambda x: x in validSet), :]\n",
    "\n",
    "# user filtering\n",
    "userFreq = ratings.groupby(['userId'])['userId'].count()\n",
    "validSet = set(userFreq.loc[userFreq >= MIN_USER_FREQ].index)\n",
    "ratings = ratings.loc[ratings['userId'].apply(lambda x: x in validSet), :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode his\n",
    "ukv, ikv = list(enumerate(ratings['userId'].unique())), list(enumerate(ratings['itemId'].unique()))\n",
    "userRawId = {encId: rawId for encId, rawId in ukv}\n",
    "userEncId = {rawId: encId for encId, rawId in ukv}\n",
    "\n",
    "# encode tar, id 0 is for padding, item encode id start from 1\n",
    "itemRawId = {encId + 1: rawId for encId, rawId in ikv}\n",
    "itemEncId = {rawId: encId + 1 for encId, rawId in ikv}\n",
    "\n",
    "# 编码\n",
    "ratings['userId'] = ratings['userId'].apply(lambda x: userEncId[x])\n",
    "ratings['itemId'] = ratings['itemId'].apply(lambda x: itemEncId[x])\n",
    "\n",
    "ratings.sort_values(by=['userId', 'timestamp'], inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "winSize = 20\n",
    "\n",
    "def padOrCut(seq, l):\n",
    "    if (len(seq) < l): return np.concatenate([seq, (l - len(seq)) * [0]])\n",
    "    # return last len\n",
    "    elif (len(seq) > l): return seq[len(seq) - l:]\n",
    "    else: return seq\n",
    "\n",
    "# split train and test users\n",
    "import random\n",
    "trainUsers, testUsers = set(), set()\n",
    "for userId in range(len(userRawId)):\n",
    "    if (random.random() <= 0.8): trainUsers.add(userId)\n",
    "    else: testUsers.add(userId)\n",
    "\n",
    "# generate user sample by sliding window\n",
    "def genUserTrainSamples(userDf):\n",
    "    userDf.reset_index(drop=True, inplace=True)\n",
    "    his, tar = [], []\n",
    "    for i in range(1, userDf.shape[0]): # enumerate y from 1\n",
    "        # x = window [i - winSize, i - 1], y = item[i]\n",
    "        his.append(padOrCut(userDf.iloc[max(0, i - winSize):i]['itemId'].values, winSize))\n",
    "        tar.append(userDf.iloc[i]['itemId'])\n",
    "\n",
    "    return np.stack(his), np.stack(tar)\n",
    "\n",
    "def genUserTestSamples(userDf):\n",
    "    userDf.reset_index(drop=True, inplace=True)\n",
    "    idx = int(0.8 * userDf.shape[0])\n",
    "    his = userDf['itemId'].iloc[:idx]\n",
    "    tar = userDf['itemId'].iloc[idx:]\n",
    "\n",
    "    return his, tar\n",
    "\n",
    "boolIdx = ratings['userId'].apply(lambda x: x in trainUsers)\n",
    "trainRatings = ratings.loc[boolIdx, :]\n",
    "testRatings = ratings.loc[~boolIdx, :]\n",
    "\n",
    "trainSamples = trainRatings.groupby('userId').apply(genUserTrainSamples)\n",
    "trainHis = np.concatenate(trainSamples.apply(lambda x: x[0]).values)\n",
    "trainTar = np.concatenate(trainSamples.apply(lambda x: x[1]).values)\n",
    "\n",
    "testSamples = testRatings.groupby('userId').apply(genUserTestSamples)\n",
    "testHis = testSamples.apply(lambda x: x[0]).values\n",
    "testTar = testSamples.apply(lambda x: x[1]).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainHis = trainHis.astype(np.int32)\n",
    "trainTar = trainTar.astype(np.int32)\n",
    "testHis = testHis.astype(np.int32)\n",
    "testTar = testTar.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/trainHis.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(trainHis))\n",
    "\n",
    "with open(\"data/trainTar.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(trainTar))\n",
    "\n",
    "with open(\"data/testHis.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(testHis))\n",
    "\n",
    "with open(\"data/testTar.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(testTar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, his, tar):\n",
    "        self.his = his\n",
    "        self.tar = tar\n",
    "        assert self.his.shape[0] == self.tar.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.his[i], self.tar[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.his.shape[0]\n",
    "\n",
    "trainData = DataLoader(\n",
    "    Dataset(trainHis, trainTar),\n",
    "    batch_size = 1028,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIND(th.nn.Module):\n",
    "    def __init__(self, D, K, R, L, nNeg, embedNum):\n",
    "        super(MIND, self).__init__()\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "        self.R = R\n",
    "        self.L = L\n",
    "        self.nNeg = nNeg\n",
    "        # weights initialization\n",
    "        self.itemEmbeds = th.nn.Embedding(embedNum, D, padding_idx=0)\n",
    "        # matmul([batch_size, k, 1, dim], [k, dim, dim']) = [batch_size, k, 1, dim']\n",
    "        self.dense1 = th.nn.Linear(D, 4 * D)\n",
    "        self.dense2 = th.nn.Linear(4 * D, D)\n",
    "        # one S for all routing operations, first dim is for batch broadcasting\n",
    "        S = th.empty(D, D)\n",
    "        th.nn.init.normal_(S, mean=0.0, std=1.0)\n",
    "        self.S = th.nn.Parameter(S) # don't forget to make S as model parameter\n",
    "        # fixed routing logits once initialized\n",
    "        self.B = th.nn.init.normal(th.empty(K, L), mean=0.0, std=1.0)\n",
    "        self.opt = th.optim.Adam(self.parameters(), lr=0.05)\n",
    "\n",
    "    # output caps' length is in (0, 1)\n",
    "    def squash(self, caps, bs):\n",
    "        n = th.norm(caps, dim=2).view(bs, self.K, 1)\n",
    "        nSquare = th.pow(n, 2)\n",
    "\n",
    "        return (nSquare / ((1 + nSquare) * n + 1e-9)) * caps\n",
    "    \n",
    "    def B2IRouting(self, his, bs):\n",
    "        \"\"\"B2I dynamic routing, input behaviors, output caps\n",
    "        \"\"\"\n",
    "        # init b, bji = b[j][i] rather than ij for matmul convinience\n",
    "        # no grad for b: https://github.com/Ugenteraan/CapsNet-PyTorch/blob/master/CapsNet-PyTorch.ipynb\n",
    "        b = self.B.detach()\n",
    "        # except for first routing round, each sample's w is different, so need a dim for batch\n",
    "        b = th.tile(b, (bs, 1, 1))\n",
    "\n",
    "        his = self.itemEmbeds(his) # (batch_size, len, dim)\n",
    "        his = th.matmul(his, self.S) # (batch_size, L, dim)\n",
    "        \n",
    "        for i in range(self.R):\n",
    "            w = th.softmax(b, dim=2)\n",
    "            if i < self.R - 1:\n",
    "                with th.no_grad():\n",
    "                    # weighted sum all i to each j\n",
    "                    caps = th.matmul(w, his) # (bs, K, D)\n",
    "                    caps = self.squash(caps, bs)\n",
    "                    b = th.matmul(th.matmul(caps, self.S), th.transpose(his, 1, 2)).detach() # (bs, K, L)\n",
    "            else:\n",
    "                caps = th.matmul(w, his) # (bs, K, D)\n",
    "                caps = self.squash(caps, bs)\n",
    "                # skip routing logits update in last round\n",
    "\n",
    "        # mlp\n",
    "        caps = self.dense2(th.relu(self.dense1(caps))) # (bs, K, D)\n",
    "        ## l2 norm\n",
    "        #caps = caps / (th.norm(caps, dim=2).view(bs, self.K, 1) + 1e-9)\n",
    "        \n",
    "        return caps\n",
    "    \n",
    "    def labelAwareAttation(self, caps, tar, p=2):\n",
    "        \"\"\"label-aware attention, input caps and targets, output logits\n",
    "            caps: (bs, K, D)\n",
    "            tar: (bs, cnt, D)\n",
    "            for postive tar, cnt = 1\n",
    "            for negative tar, cnt = self.nNeg\n",
    "        \"\"\"\n",
    "        tar = tar.transpose(1, 2) # (bs, D, cnt)\n",
    "        w = th.softmax(\n",
    "                # (bs, K, D) X (bs, D, cnt) -> (bs, K, cnt) -> (bs, cnt, K)\n",
    "                th.pow(th.transpose(th.matmul(caps, tar), 1, 2), p),\n",
    "                dim=2\n",
    "            )\n",
    "        w = w.unsqueeze(2) # (bs, cnt, K) -> (bs, cnt, 1, K)\n",
    "\n",
    "        # (bs, cnt, 1, K) X (bs, 1, K, D) -> (bs, cnt, 1, D) -> (bs, cnt, D)\n",
    "        caps = th.matmul(w, caps.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "        return caps\n",
    "\n",
    "    def sampledSoftmax(self, caps, tar, bs, tmp=0.01):\n",
    "        tarPos = self.itemEmbeds(tar) # (bs, D)\n",
    "        capsPos = self.labelAwareAttation(caps, tarPos.unsqueeze(1)).squeeze(1) # (bs, D)\n",
    "        # pos logits\n",
    "        #his = his / (th.norm(his, dim=1).view(bs, 1) + 1e-9)\n",
    "        #tar = tar / (th.norm(tar, dim=1).view(bs, 1) + 1e-9)\n",
    "        # (bs, D) dot (bs, D) -> (bs, D) - sum > (bs, )\n",
    "        posLogits = th.sigmoid(th.sum(capsPos * tarPos, dim=1) / tmp)\n",
    "\n",
    "        # neg logits\n",
    "        # in-batch negative sampling\n",
    "        tarNeg = tarPos[th.multinomial(th.ones(bs), self.nNeg * bs, replacement=True)].view(bs, self.nNeg, self.D) # (batch_size, nNeg, D)\n",
    "        capsNeg = self.labelAwareAttation(caps, tarNeg)\n",
    "        # hisNeg[b][i].dot(tarNeg[b][i]) for all b, i\n",
    "        negLogits = th.sigmoid(th.sum(capsNeg * tarNeg, dim=2).view(bs * self.nNeg) / tmp)\n",
    "\n",
    "        logits = th.concat([posLogits, negLogits])\n",
    "        labels = th.concat([th.ones(bs, ), th.zeros(bs * self.nNeg)])\n",
    "\n",
    "        return logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangyq/opt/miniconda3/envs/dgl/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Step 00000 | Loss 30.478069\n",
      "Epoch 00 | Step 00200 | Loss 2.164661\n",
      "Epoch 00 | Step 00400 | Loss 1.334919\n",
      "Epoch 00 | Step 00600 | Loss 1.061007\n",
      "Epoch 00 | Step 00800 | Loss 0.929948\n",
      "Epoch 00 | Step 01000 | Loss 0.849167\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ql/hnsc7cgj6kl1w_vdy3mjn5r40000gn/T/ipykernel_763/1852290306.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB2IRouting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampledSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ql/hnsc7cgj6kl1w_vdy3mjn5r40000gn/T/ipykernel_763/2268956346.py\u001b[0m in \u001b[0;36mB2IRouting\u001b[0;34m(self, his, bs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "model = MIND(D=8, K=3, R=3, L=winSize, nNeg=5, embedNum=len(itemEncId) + 1)\n",
    "BCELoss = th.nn.BCELoss()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epochTotalLoss = 0\n",
    "    for step, (his, tar) in enumerate(trainData):\n",
    "        bs = his.shape[0]\n",
    "        caps = model.B2IRouting(his, bs)\n",
    "        logits, labels = model.sampledSoftmax(caps, tar, bs)\n",
    "\n",
    "        loss = BCELoss(logits, labels)\n",
    "        loss.backward()\n",
    "        model.opt.step()\n",
    "        model.opt.zero_grad()\n",
    "        epochTotalLoss += loss\n",
    "        if (step % 200 == 0):\n",
    "            print('Epoch {:02d} | Step {:05d} | Loss {:.6f}'.format(\n",
    "                epoch,\n",
    "                step,\n",
    "                epochTotalLoss / (step + 1),\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@90: 0.0058033789204125245\n"
     ]
    }
   ],
   "source": [
    "testData = DataLoader(\n",
    "    Dataset(testHis, testTar),\n",
    "    batch_size = 8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "with th.no_grad():\n",
    "    ie = model.itemEmbeds.weight\n",
    "    ie /= th.norm(ie, dim=1).view(ie.shape[0], 1) + 1e-9\n",
    "    n, top = ie.shape[0], 30\n",
    "    hit, total = 0, 0\n",
    "    for his, tar in testData:\n",
    "        bs = his.shape[0]\n",
    "        caps = model.B2IRouting(his, bs)\n",
    "        logits = th.matmul(caps, th.transpose(ie, 0, 1)).detach().numpy()\n",
    "\n",
    "        # TODO: K个兴趣的所有logits和在一起取topN\n",
    "        res = np.argpartition(logits, kth=n - top, axis=2)[:, :, -top:]\n",
    "\n",
    "        for r, truth in zip(res, tar):\n",
    "            # 合并K个兴趣的召回结果\n",
    "            r = set(r.flatten())\n",
    "            if (truth.item() in r): hit += 1\n",
    "            total += 1\n",
    "\n",
    "    print(\"precision@{}: {}\".format(model.K * top, hit / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1.,  ..., 1., 1., 1.], grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.norm(ie, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型过程实验代码 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "K = 4\n",
    "R = 1\n",
    "L = 3\n",
    "\n",
    "itemEmbeds = th.nn.Embedding(len(itemEncId), D, padding_idx=0)\n",
    "\n",
    "\"\"\"\n",
    "    Get number of interest number using equation (9) in the paper\n",
    "    @x: (batch_size, seq_len), input batch user history item seq\n",
    "    @K: basic interest number\n",
    "\n",
    "    @output: (batch_size, )\n",
    "\"\"\"\n",
    "def getK(x, K):\n",
    "    return th.maximum(th.minimum(th.log2(x.count_nonzero(dim=1)), th.tensor([K])), th.tensor([1])).type(th.int8)\n",
    "\n",
    "\"\"\"\n",
    "    squash function using equation (7) in the paper\n",
    "    @caps: (batch_size, k, dim), interest capsules\n",
    "    \n",
    "    @output: (batch_size, k, dim)\n",
    "\"\"\"\n",
    "def squash(caps):\n",
    "    l2Norm = th.norm(caps, dim=2) # (batch_size, k)\n",
    "    l2NormSqure = th.pow(l2Norm, 2)\n",
    "\n",
    "    return (l2NormSqure / (1 + l2NormSqure)).view(bs, K, 1) * (caps / l2Norm.view(bs, K, 1))\n",
    "\n",
    "# weights initialization, \n",
    "# init b, bji = b[j][i]\n",
    "b = th.empty(K, L)\n",
    "th.nn.init.normal_(b, mean=0.0, std=1.0)\n",
    "# one S for all routing operations, first dim is for batch broadcasting\n",
    "S = th.empty(D, D)\n",
    "th.nn.init.normal_(S, mean=0.0, std=1.0)\n",
    "\n",
    "his = th.tensor([[1, 2, 0], [3, 2, 0]])\n",
    "tar = th.tensor([3, 1])\n",
    "batch_labels = th.tensor([1, 0])\n",
    "\n",
    "# B2I dynamic routing, input behaviors, output caps\n",
    "bs = his.shape[0]\n",
    "# k is fixed for batch forward, cannot find a way to use variant k with batch\n",
    "#k = getK(his, K) \n",
    "I = itemEmbeds(his) # (batch_size, len, dim)\n",
    "for i in range(R):\n",
    "    w = th.softmax(b, dim=1) # (K, L)\n",
    "    I = th.matmul(I, S) # (batch_size, len, dim), bilinear transform\n",
    "    caps = squash(th.matmul(w, I)) # (batch_size, K, dim)\n",
    "    _b = th.matmul(caps, th.transpose(I, 1, 2)) # (batch_size, K, L), _bji = _b[j][i]\n",
    "    # sum over batch dim first, then add to b\n",
    "    b += th.sum(_b, dim=0) # (K, L)\n",
    "\n",
    "# label-aware attention, input caps and targets, output logits\n",
    "tar = itemEmbeds(tar) # (batch_size, dim)\n",
    "# in-batch negative sampling\n",
    "\"\"\"\n",
    "pos:\n",
    "            caps                     y                  weights\n",
    "    (batch_size, K, dim) * (batch_size, dim, 1) = (batch_size, K, 1)\n",
    "\n",
    "            weights                caps\n",
    "    (batch_size, 1, K) * (batch_size, K, dim) = (batch_size, dim)\n",
    "\n",
    "neg:\n",
    "            caps                     y                  weights\n",
    "    (batch_size, K, dim) * (batch_size, dim, nNeg) = (batch_size, K, nNeg)\n",
    "\n",
    "            weights                caps\n",
    "    (batch_size, nNeg, K) * (batch_size, K, dim) = (batch_size, nNeg, dim)\n",
    "\n",
    "\"\"\"\n",
    "his = th.matmul(\n",
    "    th.softmax(\n",
    "        th.pow(th.transpose(th.matmul(caps, tar.view(bs, D, 1)), 1, 2), 2),\n",
    "        dim=2\n",
    "    ), \n",
    "    caps\n",
    ").view(bs, D) # (batch_size, dim)\n",
    "\n",
    "# pos logits\n",
    "tmp = 0.01\n",
    "his = his / th.norm(his, dim=1).view(bs, 1)\n",
    "tar = tar / th.norm(tar, dim=1).view(bs, 1)\n",
    "posLogits = th.sigmoid(th.sum(his * tar, dim=1) / tmp)\n",
    "\n",
    "# neg logits\n",
    "nNeg = 5\n",
    "tarNeg = tar[th.multinomial(th.ones(bs), nNeg * bs, replacement=True)].view(bs, nNeg, D) # (batch_size, nNeg, D)\n",
    "yNegT = th.transpose(tar[th.multinomial(th.ones(bs), nNeg * bs, replacement=True)].view(bs, nNeg, D), 1, 2) # (batch_size, D, nNeg)\n",
    "hisNeg = th.matmul(\n",
    "    th.softmax(\n",
    "        th.pow(th.transpose(th.matmul(caps, yNegT), 1, 2), 2),\n",
    "        dim=2\n",
    "    ),  # (batch_size, nNeg, K)\n",
    "    caps\n",
    ") # (batch_size, nNeg, D)\n",
    "# hisNeg[b][i].dot(tarNeg[b][i]) for all b, i\n",
    "negLogits = th.sigmoid(th.sum(hisNeg * tarNeg, dim=2).view(bs * nNeg) / tmp)\n",
    "\n",
    "logits = th.concat([posLogits, negLogits])\n",
    "labels = th.concat([th.ones(bs, ), th.zeros(bs * nNeg)])\n",
    "\n",
    "# loss\n",
    "CELoss = th.nn.BCELoss()\n",
    "loss = CELoss(logits, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "wij * Se\n",
    "\n",
    "wji is more convenient\n",
    "\n",
    "[w00, w01, w02] * each sample seq -> cap0\n",
    "[w10, w11, w12] * each sample seq -> cap1\n",
    "[w20, w21, w22] * each sample seq -> cap2\n",
    "\n",
    "\n",
    "[[w00, w01, w02]                            \n",
    " [w10, w11, w12]    *  each sample seq -> (k, dim)\n",
    " [w20, w21, w22]]\n",
    "\"\"\"\n",
    "w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7815ee787aebb751c33c27d6a55d2dad08e47ba75cac09f21612304d6f659967"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
